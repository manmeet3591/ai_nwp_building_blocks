{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8RxE0kfVwk_",
        "outputId": "637c282e-123e-49dc-98e0-ec3a981ec622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (2023.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.10/dist-packages (from xarray) (2.0.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from xarray) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->xarray) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->xarray) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->xarray) (2024.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->xarray) (1.16.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install xarray numpy torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorly"
      ],
      "metadata": {
        "id": "0k6xiYD_oQtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorly-torch"
      ],
      "metadata": {
        "id": "YRznxG1-oVY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "import numpy as np\n",
        "\n",
        "# Define the dimensions\n",
        "lat_dim = 256\n",
        "lon_dim = 512\n",
        "time_dim = 10\n",
        "\n",
        "# Create a random dataset\n",
        "data = np.random.rand(time_dim, lat_dim, lon_dim)\n",
        "\n",
        "# Create xarray dataset\n",
        "dataset = xr.DataArray(data, dims=['time', 'lat', 'lon'])\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "id": "prEmnL6BV1FJ",
        "outputId": "70c4102e-6ef4-4a64-b031-8fb7302a9de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<xarray.DataArray (time: 10, lat: 256, lon: 512)>\n",
              "array([[[0.46084708, 0.63386788, 0.34539165, ..., 0.88725927,\n",
              "         0.73438408, 0.15271957],\n",
              "        [0.92658282, 0.80314829, 0.76317385, ..., 0.65065432,\n",
              "         0.30410809, 0.6560307 ],\n",
              "        [0.59313151, 0.68012563, 0.79485986, ..., 0.24151958,\n",
              "         0.34342768, 0.1561123 ],\n",
              "        ...,\n",
              "        [0.76849875, 0.8501646 , 0.69543262, ..., 0.92553375,\n",
              "         0.91650979, 0.65617032],\n",
              "        [0.83938994, 0.42710203, 0.99728713, ..., 0.66552879,\n",
              "         0.4701873 , 0.7317872 ],\n",
              "        [0.27740142, 0.05169687, 0.81495639, ..., 0.2600541 ,\n",
              "         0.11768338, 0.75189297]],\n",
              "\n",
              "       [[0.76178947, 0.47348974, 0.77203411, ..., 0.81012709,\n",
              "         0.42722044, 0.47618152],\n",
              "        [0.51123504, 0.75732427, 0.67770609, ..., 0.39615348,\n",
              "         0.8497577 , 0.99750537],\n",
              "        [0.66401889, 0.23894993, 0.63917761, ..., 0.97828855,\n",
              "         0.29936962, 0.71476821],\n",
              "...\n",
              "        [0.49678581, 0.38681671, 0.93850167, ..., 0.58717674,\n",
              "         0.3907255 , 0.41929732],\n",
              "        [0.1455748 , 0.133957  , 0.71468396, ..., 0.61770195,\n",
              "         0.44422722, 0.15637322],\n",
              "        [0.30523805, 0.46573359, 0.99221595, ..., 0.17988622,\n",
              "         0.80725745, 0.01783473]],\n",
              "\n",
              "       [[0.92260626, 0.31764992, 0.81060791, ..., 0.77336277,\n",
              "         0.2024429 , 0.60446277],\n",
              "        [0.50133737, 0.27820624, 0.01217035, ..., 0.93243428,\n",
              "         0.03908736, 0.88658507],\n",
              "        [0.86775371, 0.66828093, 0.14973499, ..., 0.46542967,\n",
              "         0.24777149, 0.09334648],\n",
              "        ...,\n",
              "        [0.75331953, 0.12022819, 0.22581821, ..., 0.4271305 ,\n",
              "         0.25771173, 0.36573863],\n",
              "        [0.88776141, 0.9238853 , 0.46184073, ..., 0.55175865,\n",
              "         0.75223579, 0.13550002],\n",
              "        [0.20328473, 0.7974552 , 0.81823051, ..., 0.53245766,\n",
              "         0.49569745, 0.24910563]]])\n",
              "Dimensions without coordinates: time, lat, lon"
            ],
            "text/html": [
              "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
              "<defs>\n",
              "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
              "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
              "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
              "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
              "</symbol>\n",
              "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
              "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
              "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "</symbol>\n",
              "</defs>\n",
              "</svg>\n",
              "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
              " *\n",
              " */\n",
              "\n",
              ":root {\n",
              "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
              "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
              "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
              "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
              "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
              "  --xr-background-color: var(--jp-layout-color0, white);\n",
              "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
              "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
              "}\n",
              "\n",
              "html[theme=dark],\n",
              "body[data-theme=dark],\n",
              "body.vscode-dark {\n",
              "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
              "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
              "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
              "  --xr-border-color: #1F1F1F;\n",
              "  --xr-disabled-color: #515151;\n",
              "  --xr-background-color: #111111;\n",
              "  --xr-background-color-row-even: #111111;\n",
              "  --xr-background-color-row-odd: #313131;\n",
              "}\n",
              "\n",
              ".xr-wrap {\n",
              "  display: block !important;\n",
              "  min-width: 300px;\n",
              "  max-width: 700px;\n",
              "}\n",
              "\n",
              ".xr-text-repr-fallback {\n",
              "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-header {\n",
              "  padding-top: 6px;\n",
              "  padding-bottom: 6px;\n",
              "  margin-bottom: 4px;\n",
              "  border-bottom: solid 1px var(--xr-border-color);\n",
              "}\n",
              "\n",
              ".xr-header > div,\n",
              ".xr-header > ul {\n",
              "  display: inline;\n",
              "  margin-top: 0;\n",
              "  margin-bottom: 0;\n",
              "}\n",
              "\n",
              ".xr-obj-type,\n",
              ".xr-array-name {\n",
              "  margin-left: 2px;\n",
              "  margin-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-obj-type {\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-sections {\n",
              "  padding-left: 0 !important;\n",
              "  display: grid;\n",
              "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
              "}\n",
              "\n",
              ".xr-section-item {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-section-item input {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-section-item input + label {\n",
              "  color: var(--xr-disabled-color);\n",
              "}\n",
              "\n",
              ".xr-section-item input:enabled + label {\n",
              "  cursor: pointer;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-section-item input:enabled + label:hover {\n",
              "  color: var(--xr-font-color0);\n",
              "}\n",
              "\n",
              ".xr-section-summary {\n",
              "  grid-column: 1;\n",
              "  color: var(--xr-font-color2);\n",
              "  font-weight: 500;\n",
              "}\n",
              "\n",
              ".xr-section-summary > span {\n",
              "  display: inline-block;\n",
              "  padding-left: 0.5em;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:disabled + label {\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-section-summary-in + label:before {\n",
              "  display: inline-block;\n",
              "  content: '►';\n",
              "  font-size: 11px;\n",
              "  width: 15px;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:disabled + label:before {\n",
              "  color: var(--xr-disabled-color);\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked + label:before {\n",
              "  content: '▼';\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked + label > span {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-section-summary,\n",
              ".xr-section-inline-details {\n",
              "  padding-top: 4px;\n",
              "  padding-bottom: 4px;\n",
              "}\n",
              "\n",
              ".xr-section-inline-details {\n",
              "  grid-column: 2 / -1;\n",
              "}\n",
              "\n",
              ".xr-section-details {\n",
              "  display: none;\n",
              "  grid-column: 1 / -1;\n",
              "  margin-bottom: 5px;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked ~ .xr-section-details {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-array-wrap {\n",
              "  grid-column: 1 / -1;\n",
              "  display: grid;\n",
              "  grid-template-columns: 20px auto;\n",
              "}\n",
              "\n",
              ".xr-array-wrap > label {\n",
              "  grid-column: 1;\n",
              "  vertical-align: top;\n",
              "}\n",
              "\n",
              ".xr-preview {\n",
              "  color: var(--xr-font-color3);\n",
              "}\n",
              "\n",
              ".xr-array-preview,\n",
              ".xr-array-data {\n",
              "  padding: 0 5px !important;\n",
              "  grid-column: 2;\n",
              "}\n",
              "\n",
              ".xr-array-data,\n",
              ".xr-array-in:checked ~ .xr-array-preview {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-array-in:checked ~ .xr-array-data,\n",
              ".xr-array-preview {\n",
              "  display: inline-block;\n",
              "}\n",
              "\n",
              ".xr-dim-list {\n",
              "  display: inline-block !important;\n",
              "  list-style: none;\n",
              "  padding: 0 !important;\n",
              "  margin: 0;\n",
              "}\n",
              "\n",
              ".xr-dim-list li {\n",
              "  display: inline-block;\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "}\n",
              "\n",
              ".xr-dim-list:before {\n",
              "  content: '(';\n",
              "}\n",
              "\n",
              ".xr-dim-list:after {\n",
              "  content: ')';\n",
              "}\n",
              "\n",
              ".xr-dim-list li:not(:last-child):after {\n",
              "  content: ',';\n",
              "  padding-right: 5px;\n",
              "}\n",
              "\n",
              ".xr-has-index {\n",
              "  font-weight: bold;\n",
              "}\n",
              "\n",
              ".xr-var-list,\n",
              ".xr-var-item {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-var-item > div,\n",
              ".xr-var-item label,\n",
              ".xr-var-item > .xr-var-name span {\n",
              "  background-color: var(--xr-background-color-row-even);\n",
              "  margin-bottom: 0;\n",
              "}\n",
              "\n",
              ".xr-var-item > .xr-var-name:hover span {\n",
              "  padding-right: 5px;\n",
              "}\n",
              "\n",
              ".xr-var-list > li:nth-child(odd) > div,\n",
              ".xr-var-list > li:nth-child(odd) > label,\n",
              ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
              "  background-color: var(--xr-background-color-row-odd);\n",
              "}\n",
              "\n",
              ".xr-var-name {\n",
              "  grid-column: 1;\n",
              "}\n",
              "\n",
              ".xr-var-dims {\n",
              "  grid-column: 2;\n",
              "}\n",
              "\n",
              ".xr-var-dtype {\n",
              "  grid-column: 3;\n",
              "  text-align: right;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-var-preview {\n",
              "  grid-column: 4;\n",
              "}\n",
              "\n",
              ".xr-index-preview {\n",
              "  grid-column: 2 / 5;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-var-name,\n",
              ".xr-var-dims,\n",
              ".xr-var-dtype,\n",
              ".xr-preview,\n",
              ".xr-attrs dt {\n",
              "  white-space: nowrap;\n",
              "  overflow: hidden;\n",
              "  text-overflow: ellipsis;\n",
              "  padding-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-var-name:hover,\n",
              ".xr-var-dims:hover,\n",
              ".xr-var-dtype:hover,\n",
              ".xr-attrs dt:hover {\n",
              "  overflow: visible;\n",
              "  width: auto;\n",
              "  z-index: 1;\n",
              "}\n",
              "\n",
              ".xr-var-attrs,\n",
              ".xr-var-data,\n",
              ".xr-index-data {\n",
              "  display: none;\n",
              "  background-color: var(--xr-background-color) !important;\n",
              "  padding-bottom: 5px !important;\n",
              "}\n",
              "\n",
              ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
              ".xr-var-data-in:checked ~ .xr-var-data,\n",
              ".xr-index-data-in:checked ~ .xr-index-data {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              ".xr-var-data > table {\n",
              "  float: right;\n",
              "}\n",
              "\n",
              ".xr-var-name span,\n",
              ".xr-var-data,\n",
              ".xr-index-name div,\n",
              ".xr-index-data,\n",
              ".xr-attrs {\n",
              "  padding-left: 25px !important;\n",
              "}\n",
              "\n",
              ".xr-attrs,\n",
              ".xr-var-attrs,\n",
              ".xr-var-data,\n",
              ".xr-index-data {\n",
              "  grid-column: 1 / -1;\n",
              "}\n",
              "\n",
              "dl.xr-attrs {\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "  display: grid;\n",
              "  grid-template-columns: 125px auto;\n",
              "}\n",
              "\n",
              ".xr-attrs dt,\n",
              ".xr-attrs dd {\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "  float: left;\n",
              "  padding-right: 10px;\n",
              "  width: auto;\n",
              "}\n",
              "\n",
              ".xr-attrs dt {\n",
              "  font-weight: normal;\n",
              "  grid-column: 1;\n",
              "}\n",
              "\n",
              ".xr-attrs dt:hover span {\n",
              "  display: inline-block;\n",
              "  background: var(--xr-background-color);\n",
              "  padding-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-attrs dd {\n",
              "  grid-column: 2;\n",
              "  white-space: pre-wrap;\n",
              "  word-break: break-all;\n",
              "}\n",
              "\n",
              ".xr-icon-database,\n",
              ".xr-icon-file-text2,\n",
              ".xr-no-icon {\n",
              "  display: inline-block;\n",
              "  vertical-align: middle;\n",
              "  width: 1em;\n",
              "  height: 1.5em !important;\n",
              "  stroke-width: 0;\n",
              "  stroke: currentColor;\n",
              "  fill: currentColor;\n",
              "}\n",
              "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray (time: 10, lat: 256, lon: 512)&gt;\n",
              "array([[[0.46084708, 0.63386788, 0.34539165, ..., 0.88725927,\n",
              "         0.73438408, 0.15271957],\n",
              "        [0.92658282, 0.80314829, 0.76317385, ..., 0.65065432,\n",
              "         0.30410809, 0.6560307 ],\n",
              "        [0.59313151, 0.68012563, 0.79485986, ..., 0.24151958,\n",
              "         0.34342768, 0.1561123 ],\n",
              "        ...,\n",
              "        [0.76849875, 0.8501646 , 0.69543262, ..., 0.92553375,\n",
              "         0.91650979, 0.65617032],\n",
              "        [0.83938994, 0.42710203, 0.99728713, ..., 0.66552879,\n",
              "         0.4701873 , 0.7317872 ],\n",
              "        [0.27740142, 0.05169687, 0.81495639, ..., 0.2600541 ,\n",
              "         0.11768338, 0.75189297]],\n",
              "\n",
              "       [[0.76178947, 0.47348974, 0.77203411, ..., 0.81012709,\n",
              "         0.42722044, 0.47618152],\n",
              "        [0.51123504, 0.75732427, 0.67770609, ..., 0.39615348,\n",
              "         0.8497577 , 0.99750537],\n",
              "        [0.66401889, 0.23894993, 0.63917761, ..., 0.97828855,\n",
              "         0.29936962, 0.71476821],\n",
              "...\n",
              "        [0.49678581, 0.38681671, 0.93850167, ..., 0.58717674,\n",
              "         0.3907255 , 0.41929732],\n",
              "        [0.1455748 , 0.133957  , 0.71468396, ..., 0.61770195,\n",
              "         0.44422722, 0.15637322],\n",
              "        [0.30523805, 0.46573359, 0.99221595, ..., 0.17988622,\n",
              "         0.80725745, 0.01783473]],\n",
              "\n",
              "       [[0.92260626, 0.31764992, 0.81060791, ..., 0.77336277,\n",
              "         0.2024429 , 0.60446277],\n",
              "        [0.50133737, 0.27820624, 0.01217035, ..., 0.93243428,\n",
              "         0.03908736, 0.88658507],\n",
              "        [0.86775371, 0.66828093, 0.14973499, ..., 0.46542967,\n",
              "         0.24777149, 0.09334648],\n",
              "        ...,\n",
              "        [0.75331953, 0.12022819, 0.22581821, ..., 0.4271305 ,\n",
              "         0.25771173, 0.36573863],\n",
              "        [0.88776141, 0.9238853 , 0.46184073, ..., 0.55175865,\n",
              "         0.75223579, 0.13550002],\n",
              "        [0.20328473, 0.7974552 , 0.81823051, ..., 0.53245766,\n",
              "         0.49569745, 0.24910563]]])\n",
              "Dimensions without coordinates: time, lat, lon</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'></div><ul class='xr-dim-list'><li><span>time</span>: 10</li><li><span>lat</span>: 256</li><li><span>lon</span>: 512</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-136d51ec-719c-4501-a69b-0b61159d3b7b' class='xr-array-in' type='checkbox' checked><label for='section-136d51ec-719c-4501-a69b-0b61159d3b7b' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>0.4608 0.6339 0.3454 0.1584 0.5683 ... 0.2406 0.5325 0.4957 0.2491</span></div><div class='xr-array-data'><pre>array([[[0.46084708, 0.63386788, 0.34539165, ..., 0.88725927,\n",
              "         0.73438408, 0.15271957],\n",
              "        [0.92658282, 0.80314829, 0.76317385, ..., 0.65065432,\n",
              "         0.30410809, 0.6560307 ],\n",
              "        [0.59313151, 0.68012563, 0.79485986, ..., 0.24151958,\n",
              "         0.34342768, 0.1561123 ],\n",
              "        ...,\n",
              "        [0.76849875, 0.8501646 , 0.69543262, ..., 0.92553375,\n",
              "         0.91650979, 0.65617032],\n",
              "        [0.83938994, 0.42710203, 0.99728713, ..., 0.66552879,\n",
              "         0.4701873 , 0.7317872 ],\n",
              "        [0.27740142, 0.05169687, 0.81495639, ..., 0.2600541 ,\n",
              "         0.11768338, 0.75189297]],\n",
              "\n",
              "       [[0.76178947, 0.47348974, 0.77203411, ..., 0.81012709,\n",
              "         0.42722044, 0.47618152],\n",
              "        [0.51123504, 0.75732427, 0.67770609, ..., 0.39615348,\n",
              "         0.8497577 , 0.99750537],\n",
              "        [0.66401889, 0.23894993, 0.63917761, ..., 0.97828855,\n",
              "         0.29936962, 0.71476821],\n",
              "...\n",
              "        [0.49678581, 0.38681671, 0.93850167, ..., 0.58717674,\n",
              "         0.3907255 , 0.41929732],\n",
              "        [0.1455748 , 0.133957  , 0.71468396, ..., 0.61770195,\n",
              "         0.44422722, 0.15637322],\n",
              "        [0.30523805, 0.46573359, 0.99221595, ..., 0.17988622,\n",
              "         0.80725745, 0.01783473]],\n",
              "\n",
              "       [[0.92260626, 0.31764992, 0.81060791, ..., 0.77336277,\n",
              "         0.2024429 , 0.60446277],\n",
              "        [0.50133737, 0.27820624, 0.01217035, ..., 0.93243428,\n",
              "         0.03908736, 0.88658507],\n",
              "        [0.86775371, 0.66828093, 0.14973499, ..., 0.46542967,\n",
              "         0.24777149, 0.09334648],\n",
              "        ...,\n",
              "        [0.75331953, 0.12022819, 0.22581821, ..., 0.4271305 ,\n",
              "         0.25771173, 0.36573863],\n",
              "        [0.88776141, 0.9238853 , 0.46184073, ..., 0.55175865,\n",
              "         0.75223579, 0.13550002],\n",
              "        [0.20328473, 0.7974552 , 0.81823051, ..., 0.53245766,\n",
              "         0.49569745, 0.24910563]]])</pre></div></div></li><li class='xr-section-item'><input id='section-6daf3141-77de-43be-9476-fa482522acd9' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-6daf3141-77de-43be-9476-fa482522acd9' class='xr-section-summary'  title='Expand/collapse section'>Coordinates: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-8b828c41-4a68-4813-af24-257478520bd3' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-8b828c41-4a68-4813-af24-257478520bd3' class='xr-section-summary'  title='Expand/collapse section'>Indexes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-6544bac2-f856-469d-98e0-871915b6c89d' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-6544bac2-f856-469d-98e0-871915b6c89d' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-harmonics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw5ClWjOb-dR",
        "outputId": "aeff16fb-c119-4ca5-c0ba-2c94141622f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-harmonics\n",
            "  Downloading torch_harmonics-0.6.5-py3-none-any.whl (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch-harmonics) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-harmonics) (1.25.2)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from torch-harmonics) (2.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torch-harmonics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torch-harmonics) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch-harmonics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torch-harmonics) (1.3.0)\n",
            "Installing collected packages: torch-harmonics\n",
            "Successfully installed torch-harmonics-0.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_harmonics import RealSHT, InverseRealSHT\n",
        "\n",
        "# Convert xarray dataset to torch tensor\n",
        "data_tensor = torch.tensor(dataset.values, dtype=torch.float32)\n",
        "\n",
        "# Define dimensions\n",
        "nlat, nlon = lat_dim, lon_dim\n",
        "\n",
        "# Initialize the SHT and Inverse SHT\n",
        "sht = RealSHT(nlat, nlon, lmax=nlat//2, mmax=nlon//2, grid='equiangular').to(torch.float32)\n",
        "inverse_sht = InverseRealSHT(nlat, nlon, lmax=nlat//2, mmax=nlon//2, grid='equiangular').to(torch.float32)\n",
        "\n",
        "# Perform the SHT (Fourier Transform)\n",
        "data_transformed = sht(data_tensor)\n",
        "print(\"Transformed data shape:\", data_transformed.shape)\n",
        "\n",
        "# NOTE: In a real application, you'd apply your SFNO model here, which would operate on the `data_transformed`."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzmMIVUzWBjZ",
        "outputId": "6a7ce9cf-ef3e-4ca9-dcc1-536573a8164a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed data shape: torch.Size([10, 128, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model here - preferably Swin Transformer or Diffusion taking t, t-1 as input and t+1 as target"
      ],
      "metadata": {
        "id": "rWvxBxSFdGjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the inverse SHT (Inverse Fourier Transform)\n",
        "data_inverse_transformed = inverse_sht(data_transformed)\n",
        "print(\"Inverse Transformed data shape:\", data_inverse_transformed.shape)\n",
        "\n",
        "# Convert back to xarray dataset\n",
        "dataset_inverse = xr.DataArray(data_inverse_transformed.cpu().numpy(), dims=['time', 'lat', 'lon'])\n",
        "print(dataset_inverse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INQziFL_WGMA",
        "outputId": "fd39b716-57b6-4932-dfd4-8f685a90b5a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inverse Transformed data shape: torch.Size([10, 256, 512])\n",
            "<xarray.DataArray (time: 10, lat: 256, lon: 512)>\n",
            "array([[[0.48245522, 0.48245522, 0.48245522, ..., 0.48245522,\n",
            "         0.48245522, 0.48245522],\n",
            "        [0.50700676, 0.50730795, 0.5076015 , ..., 0.50605804,\n",
            "         0.5063817 , 0.50669795],\n",
            "        [0.5372217 , 0.5375544 , 0.5378629 , ..., 0.5360825 ,\n",
            "         0.5364854 , 0.53686523],\n",
            "        ...,\n",
            "        [0.5500696 , 0.54969645, 0.5493152 , ..., 0.5511424 ,\n",
            "         0.5507923 , 0.55043477],\n",
            "        [0.5205016 , 0.5202193 , 0.51992565, ..., 0.52127874,\n",
            "         0.5210314 , 0.52077234],\n",
            "        [0.4825165 , 0.4825165 , 0.4825165 , ..., 0.4825165 ,\n",
            "         0.4825165 , 0.4825165 ]],\n",
            "\n",
            "       [[0.54244804, 0.54244804, 0.54244804, ..., 0.54244804,\n",
            "         0.54244804, 0.54244804],\n",
            "        [0.53471065, 0.53479004, 0.53486544, ..., 0.5344479 ,\n",
            "         0.5345395 , 0.53462714],\n",
            "        [0.5174676 , 0.5174264 , 0.5173575 , ..., 0.51742345,\n",
            "         0.51746637, 0.517481  ],\n",
            "...\n",
            "        [0.5010833 , 0.502073  , 0.50303245, ..., 0.4979437 ,\n",
            "         0.49901757, 0.50006455],\n",
            "        [0.4797827 , 0.48041645, 0.4810502 , ..., 0.47788393,\n",
            "         0.47851622, 0.47914922],\n",
            "        [0.49510235, 0.49510235, 0.49510235, ..., 0.49510235,\n",
            "         0.49510235, 0.49510235]],\n",
            "\n",
            "       [[0.4887052 , 0.4887052 , 0.4887052 , ..., 0.4887052 ,\n",
            "         0.4887052 , 0.4887052 ],\n",
            "        [0.51410574, 0.5141065 , 0.5140994 , ..., 0.5140556 ,\n",
            "         0.5140803 , 0.514097  ],\n",
            "        [0.51407754, 0.5142145 , 0.514342  , ..., 0.5136105 ,\n",
            "         0.5137754 , 0.5139311 ],\n",
            "        ...,\n",
            "        [0.41826907, 0.41753942, 0.4168614 , ..., 0.4207559 ,\n",
            "         0.41987848, 0.4190491 ],\n",
            "        [0.46520272, 0.46484506, 0.46450576, ..., 0.46638408,\n",
            "         0.46597248, 0.4655786 ],\n",
            "        [0.50530237, 0.50530237, 0.50530237, ..., 0.50530237,\n",
            "         0.50530237, 0.50530237]]], dtype=float32)\n",
            "Dimensions without coordinates: time, lat, lon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_harmonics import RealSHT, InverseRealSHT\n",
        "\n",
        "# Step 1: Generate a random xarray dataset\n",
        "lat_dim = 256\n",
        "lon_dim = 512\n",
        "time_dim = 10\n",
        "data = np.random.rand(time_dim, lat_dim, lon_dim)\n",
        "dataset = xr.DataArray(data, dims=['time', 'lat', 'lon'])\n",
        "# print(\"Original Dataset:\")\n",
        "# print(dataset)\n",
        "\n",
        "# Step 2: Transform it to the Fourier domain using SFNO\n",
        "data_tensor = torch.tensor(dataset.values, dtype=torch.float32)\n",
        "nlat, nlon = lat_dim, lon_dim\n",
        "sht = RealSHT(nlat, nlon, lmax=nlat//2, mmax=nlon//2, grid='equiangular').to(torch.float32)\n",
        "inverse_sht = InverseRealSHT(nlat, nlon, lmax=nlat//2, mmax=nlon//2, grid='equiangular').to(torch.float32)\n",
        "data_transformed = sht(data_tensor)\n",
        "print(\"Transformed Data Shape:\", data_transformed.shape)\n",
        "\n",
        "# Step 3: Apply an inverse transform to get it back to the spatial domain\n",
        "data_inverse_transformed = inverse_sht(data_transformed)\n",
        "print(\"Inverse Transformed Data Shape:\", data_inverse_transformed.shape)\n",
        "\n",
        "# Convert back to xarray dataset\n",
        "dataset_inverse = xr.DataArray(data_inverse_transformed.cpu().numpy(), dims=['time', 'lat', 'lon'])\n",
        "# print(\"Inverse Transformed Dataset:\")\n",
        "# print(dataset_inverse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhu7wbXQcQHp",
        "outputId": "f8f08787-0012-40b9-e81d-a52c9a070262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed Data Shape: torch.Size([10, 128, 256])\n",
            "Inverse Transformed Data Shape: torch.Size([10, 256, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_transformed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqhmukA1dLZC",
        "outputId": "793977a9-7c94-4e38-b395-914d50733bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.7747e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [-9.7422e-04+0.0000e+00j, -3.4883e-04+1.4136e-04j,\n",
              "           0.0000e+00+0.0000e+00j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 2.8978e-03+0.0000e+00j,  1.2435e-03-2.6684e-03j,\n",
              "          -4.0910e-05+1.7434e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         ...,\n",
              "         [-4.5699e-03+0.0000e+00j,  4.3338e-04+1.1155e-03j,\n",
              "           1.3477e-04-1.7609e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 2.7227e-04+0.0000e+00j,  4.0801e-04+1.2671e-03j,\n",
              "          -4.2849e-03+3.4344e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 4.9748e-03+0.0000e+00j,  5.2963e-04+1.4065e-03j,\n",
              "           1.2269e-03-3.0374e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j]],\n",
              "\n",
              "        [[ 1.7720e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [-3.6587e-03+0.0000e+00j,  5.1000e-03-2.7481e-03j,\n",
              "           0.0000e+00+0.0000e+00j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [-1.0989e-03+0.0000e+00j,  3.7341e-03-2.5250e-03j,\n",
              "          -4.1191e-04-3.2761e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         ...,\n",
              "         [ 4.9343e-05+0.0000e+00j,  2.3528e-03-4.4951e-03j,\n",
              "           1.4750e-04+2.4334e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [-8.0171e-04+0.0000e+00j,  1.3745e-03+2.2006e-03j,\n",
              "           1.8130e-03+3.6531e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [-3.5725e-03+0.0000e+00j, -5.0884e-04+1.1418e-03j,\n",
              "           1.5489e-03-4.6613e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j]],\n",
              "\n",
              "        [[ 1.7680e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 1.4916e-03+0.0000e+00j, -3.3347e-06+1.9663e-03j,\n",
              "           0.0000e+00+0.0000e+00j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 1.2501e-03+0.0000e+00j,  1.2612e-03+3.9177e-03j,\n",
              "          -3.5243e-03+3.9373e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         ...,\n",
              "         [-1.0823e-03+0.0000e+00j, -3.3667e-03-9.5094e-04j,\n",
              "           4.0429e-04+8.0477e-04j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 2.1341e-03+0.0000e+00j,  3.2742e-03-1.2037e-05j,\n",
              "          -1.0070e-03-8.8341e-04j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [-1.1518e-03+0.0000e+00j,  6.8963e-04+1.4015e-03j,\n",
              "           3.7785e-03-2.4808e-04j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 1.7695e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 3.1196e-03+0.0000e+00j,  1.3351e-03+2.5534e-05j,\n",
              "           0.0000e+00+0.0000e+00j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 5.0462e-03+0.0000e+00j,  2.8744e-05+8.2893e-04j,\n",
              "          -3.1678e-04+1.8523e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         ...,\n",
              "         [-1.2764e-03+0.0000e+00j, -2.9391e-03-2.1322e-03j,\n",
              "          -9.6715e-04-3.0256e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [-7.6852e-04+0.0000e+00j, -9.0208e-04-3.2387e-04j,\n",
              "          -2.3916e-03+9.4486e-04j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 2.3948e-03+0.0000e+00j,  1.5846e-03+4.8109e-03j,\n",
              "          -8.9422e-04+3.2354e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j]],\n",
              "\n",
              "        [[ 1.7713e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 2.4383e-03+0.0000e+00j,  2.2057e-03-2.0598e-03j,\n",
              "           0.0000e+00+0.0000e+00j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 8.0550e-04+0.0000e+00j, -9.9595e-05-1.7601e-03j,\n",
              "          -2.1210e-03+4.3856e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         ...,\n",
              "         [-3.5021e-03+0.0000e+00j,  3.6108e-04+1.4123e-04j,\n",
              "           3.1109e-04+1.8309e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 2.2809e-03+0.0000e+00j,  6.1572e-04-2.9688e-04j,\n",
              "           1.6288e-03-1.3665e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 1.1206e-03+0.0000e+00j, -1.4217e-03-4.1004e-04j,\n",
              "          -2.6333e-03-2.8194e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j]],\n",
              "\n",
              "        [[ 1.7703e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [-1.3220e-03+0.0000e+00j, -5.5558e-03-9.2524e-04j,\n",
              "           0.0000e+00+0.0000e+00j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 1.6283e-03+0.0000e+00j, -3.1374e-03-1.6165e-03j,\n",
              "           2.6449e-03+1.1069e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         ...,\n",
              "         [-4.7291e-03+0.0000e+00j,  1.1336e-03+2.8537e-03j,\n",
              "          -1.1823e-03-1.5991e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [ 4.1269e-04+0.0000e+00j,  5.1404e-05+2.5808e-04j,\n",
              "           1.5823e-03+6.1523e-04j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j],\n",
              "         [-2.1800e-03+0.0000e+00j, -4.6995e-04-2.1672e-03j,\n",
              "           1.7354e-03+1.4778e-03j,  ...,\n",
              "           0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,\n",
              "           0.0000e+00+0.0000e+00j]]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda import amp\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "from math import ceil, sqrt\n",
        "\n",
        "import time\n",
        "\n",
        "cmap='twilight_shifted'"
      ],
      "metadata": {
        "id": "ybngwDladTxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enable_amp = False\n",
        "\n",
        "# set device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.set_device(device.index)"
      ],
      "metadata": {
        "id": "oISCtXxodiX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/torch-harmonics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvF-bpTpjbwr",
        "outputId": "f46cb8d9-ed3d-4978-ac1d-bdf6eef4efe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torch-harmonics'...\n",
            "remote: Enumerating objects: 904, done.\u001b[K\n",
            "remote: Counting objects: 100% (504/504), done.\u001b[K\n",
            "remote: Compressing objects: 100% (191/191), done.\u001b[K\n",
            "remote: Total 904 (delta 377), reused 372 (delta 303), pack-reused 400\u001b[K\n",
            "Receiving objects: 100% (904/904), 8.13 MiB | 27.57 MiB/s, done.\n",
            "Resolving deltas: 100% (574/574), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls torch-harmonics/examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_F3jCV0jh5j",
        "outputId": "10571890-3f42-4866-f550-e553767a4f1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimal_example.py  train_sfno.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset\n",
        "# from torch_harmonics.examples.train_sfno import PdeDataset\n",
        "\n",
        "# 1 hour prediction steps\n",
        "dt = 1*3600\n",
        "dt_solver = 150\n",
        "nsteps = dt//dt_solver\n",
        "\n",
        "# Create a synthetic dataset with random data\n",
        "data = np.random.rand(time_dim, nlat, nlon)\n",
        "dataset = xr.DataArray(data, dims=['time', 'lat', 'lon'])\n",
        "print(\"Original Dataset:\")\n",
        "print(dataset)\n",
        "\n",
        "# dataset = PdeDataset(dt=dt, nsteps=nsteps, dims=(256, 512), device=device, normalize=True)\n",
        "# There is still an issue with parallel dataloading. Do NOT use it at the moment\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0, persistent_workers=False)\n",
        "# solver = dataset.solver.to(device)\n",
        "\n",
        "nlat = dataset.lat.values.shape[0]\n",
        "nlon = dataset.lon.values.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuOypx28jP8X",
        "outputId": "97f45728-3774-4b1f-ded2-c85eb2c49e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "<xarray.DataArray (time: 10, lat: 256, lon: 512)>\n",
            "array([[[0.13664545, 0.32887597, 0.16400457, ..., 0.78938287,\n",
            "         0.24562398, 0.48045371],\n",
            "        [0.66315231, 0.70478573, 0.73073083, ..., 0.75658078,\n",
            "         0.74332593, 0.13430482],\n",
            "        [0.7173843 , 0.77349078, 0.67879202, ..., 0.31228275,\n",
            "         0.04309166, 0.61334845],\n",
            "        ...,\n",
            "        [0.06139518, 0.31531274, 0.42670884, ..., 0.93641654,\n",
            "         0.1011765 , 0.59698909],\n",
            "        [0.30094224, 0.09741645, 0.51358164, ..., 0.89605468,\n",
            "         0.42517612, 0.1787893 ],\n",
            "        [0.33244633, 0.5506997 , 0.46219064, ..., 0.17560451,\n",
            "         0.77574484, 0.89141205]],\n",
            "\n",
            "       [[0.35372644, 0.58949174, 0.33371672, ..., 0.23841651,\n",
            "         0.4197214 , 0.0418784 ],\n",
            "        [0.77661124, 0.14170365, 0.56176736, ..., 0.40929684,\n",
            "         0.27586892, 0.46166271],\n",
            "        [0.44123109, 0.08398815, 0.25544012, ..., 0.40527196,\n",
            "         0.04522792, 0.18044354],\n",
            "...\n",
            "        [0.00550026, 0.27413701, 0.56717994, ..., 0.83236285,\n",
            "         0.25179192, 0.19902724],\n",
            "        [0.4871888 , 0.86833526, 0.55141899, ..., 0.78331999,\n",
            "         0.42362269, 0.90254593],\n",
            "        [0.39569856, 0.57417873, 0.76809184, ..., 0.74587134,\n",
            "         0.13068811, 0.25544508]],\n",
            "\n",
            "       [[0.9470806 , 0.98569814, 0.16229052, ..., 0.37357493,\n",
            "         0.93713055, 0.82920746],\n",
            "        [0.69256943, 0.04596307, 0.70782498, ..., 0.15126597,\n",
            "         0.74277918, 0.34218779],\n",
            "        [0.58955714, 0.09528321, 0.16638967, ..., 0.88429185,\n",
            "         0.78529235, 0.38035069],\n",
            "        ...,\n",
            "        [0.13426855, 0.32683767, 0.92079097, ..., 0.07083984,\n",
            "         0.45406553, 0.55651957],\n",
            "        [0.3212409 , 0.89754804, 0.50476702, ..., 0.20709826,\n",
            "         0.35124961, 0.30008607],\n",
            "        [0.67135585, 0.05002323, 0.81442564, ..., 0.60543809,\n",
            "         0.18766374, 0.41956721]]])\n",
            "Dimensions without coordinates: time, lat, lon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "id": "7gmLcXk5kUna",
        "outputId": "550b03d8-8e41-475c-c92d-2e0913087d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<xarray.DataArray (time: 10, lat: 256, lon: 512)>\n",
              "array([[[0.13664545, 0.32887597, 0.16400457, ..., 0.78938287,\n",
              "         0.24562398, 0.48045371],\n",
              "        [0.66315231, 0.70478573, 0.73073083, ..., 0.75658078,\n",
              "         0.74332593, 0.13430482],\n",
              "        [0.7173843 , 0.77349078, 0.67879202, ..., 0.31228275,\n",
              "         0.04309166, 0.61334845],\n",
              "        ...,\n",
              "        [0.06139518, 0.31531274, 0.42670884, ..., 0.93641654,\n",
              "         0.1011765 , 0.59698909],\n",
              "        [0.30094224, 0.09741645, 0.51358164, ..., 0.89605468,\n",
              "         0.42517612, 0.1787893 ],\n",
              "        [0.33244633, 0.5506997 , 0.46219064, ..., 0.17560451,\n",
              "         0.77574484, 0.89141205]],\n",
              "\n",
              "       [[0.35372644, 0.58949174, 0.33371672, ..., 0.23841651,\n",
              "         0.4197214 , 0.0418784 ],\n",
              "        [0.77661124, 0.14170365, 0.56176736, ..., 0.40929684,\n",
              "         0.27586892, 0.46166271],\n",
              "        [0.44123109, 0.08398815, 0.25544012, ..., 0.40527196,\n",
              "         0.04522792, 0.18044354],\n",
              "...\n",
              "        [0.00550026, 0.27413701, 0.56717994, ..., 0.83236285,\n",
              "         0.25179192, 0.19902724],\n",
              "        [0.4871888 , 0.86833526, 0.55141899, ..., 0.78331999,\n",
              "         0.42362269, 0.90254593],\n",
              "        [0.39569856, 0.57417873, 0.76809184, ..., 0.74587134,\n",
              "         0.13068811, 0.25544508]],\n",
              "\n",
              "       [[0.9470806 , 0.98569814, 0.16229052, ..., 0.37357493,\n",
              "         0.93713055, 0.82920746],\n",
              "        [0.69256943, 0.04596307, 0.70782498, ..., 0.15126597,\n",
              "         0.74277918, 0.34218779],\n",
              "        [0.58955714, 0.09528321, 0.16638967, ..., 0.88429185,\n",
              "         0.78529235, 0.38035069],\n",
              "        ...,\n",
              "        [0.13426855, 0.32683767, 0.92079097, ..., 0.07083984,\n",
              "         0.45406553, 0.55651957],\n",
              "        [0.3212409 , 0.89754804, 0.50476702, ..., 0.20709826,\n",
              "         0.35124961, 0.30008607],\n",
              "        [0.67135585, 0.05002323, 0.81442564, ..., 0.60543809,\n",
              "         0.18766374, 0.41956721]]])\n",
              "Dimensions without coordinates: time, lat, lon"
            ],
            "text/html": [
              "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
              "<defs>\n",
              "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
              "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
              "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
              "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
              "</symbol>\n",
              "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
              "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
              "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "</symbol>\n",
              "</defs>\n",
              "</svg>\n",
              "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
              " *\n",
              " */\n",
              "\n",
              ":root {\n",
              "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
              "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
              "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
              "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
              "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
              "  --xr-background-color: var(--jp-layout-color0, white);\n",
              "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
              "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
              "}\n",
              "\n",
              "html[theme=dark],\n",
              "body[data-theme=dark],\n",
              "body.vscode-dark {\n",
              "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
              "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
              "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
              "  --xr-border-color: #1F1F1F;\n",
              "  --xr-disabled-color: #515151;\n",
              "  --xr-background-color: #111111;\n",
              "  --xr-background-color-row-even: #111111;\n",
              "  --xr-background-color-row-odd: #313131;\n",
              "}\n",
              "\n",
              ".xr-wrap {\n",
              "  display: block !important;\n",
              "  min-width: 300px;\n",
              "  max-width: 700px;\n",
              "}\n",
              "\n",
              ".xr-text-repr-fallback {\n",
              "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-header {\n",
              "  padding-top: 6px;\n",
              "  padding-bottom: 6px;\n",
              "  margin-bottom: 4px;\n",
              "  border-bottom: solid 1px var(--xr-border-color);\n",
              "}\n",
              "\n",
              ".xr-header > div,\n",
              ".xr-header > ul {\n",
              "  display: inline;\n",
              "  margin-top: 0;\n",
              "  margin-bottom: 0;\n",
              "}\n",
              "\n",
              ".xr-obj-type,\n",
              ".xr-array-name {\n",
              "  margin-left: 2px;\n",
              "  margin-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-obj-type {\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-sections {\n",
              "  padding-left: 0 !important;\n",
              "  display: grid;\n",
              "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
              "}\n",
              "\n",
              ".xr-section-item {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-section-item input {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-section-item input + label {\n",
              "  color: var(--xr-disabled-color);\n",
              "}\n",
              "\n",
              ".xr-section-item input:enabled + label {\n",
              "  cursor: pointer;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-section-item input:enabled + label:hover {\n",
              "  color: var(--xr-font-color0);\n",
              "}\n",
              "\n",
              ".xr-section-summary {\n",
              "  grid-column: 1;\n",
              "  color: var(--xr-font-color2);\n",
              "  font-weight: 500;\n",
              "}\n",
              "\n",
              ".xr-section-summary > span {\n",
              "  display: inline-block;\n",
              "  padding-left: 0.5em;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:disabled + label {\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-section-summary-in + label:before {\n",
              "  display: inline-block;\n",
              "  content: '►';\n",
              "  font-size: 11px;\n",
              "  width: 15px;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:disabled + label:before {\n",
              "  color: var(--xr-disabled-color);\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked + label:before {\n",
              "  content: '▼';\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked + label > span {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-section-summary,\n",
              ".xr-section-inline-details {\n",
              "  padding-top: 4px;\n",
              "  padding-bottom: 4px;\n",
              "}\n",
              "\n",
              ".xr-section-inline-details {\n",
              "  grid-column: 2 / -1;\n",
              "}\n",
              "\n",
              ".xr-section-details {\n",
              "  display: none;\n",
              "  grid-column: 1 / -1;\n",
              "  margin-bottom: 5px;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked ~ .xr-section-details {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-array-wrap {\n",
              "  grid-column: 1 / -1;\n",
              "  display: grid;\n",
              "  grid-template-columns: 20px auto;\n",
              "}\n",
              "\n",
              ".xr-array-wrap > label {\n",
              "  grid-column: 1;\n",
              "  vertical-align: top;\n",
              "}\n",
              "\n",
              ".xr-preview {\n",
              "  color: var(--xr-font-color3);\n",
              "}\n",
              "\n",
              ".xr-array-preview,\n",
              ".xr-array-data {\n",
              "  padding: 0 5px !important;\n",
              "  grid-column: 2;\n",
              "}\n",
              "\n",
              ".xr-array-data,\n",
              ".xr-array-in:checked ~ .xr-array-preview {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-array-in:checked ~ .xr-array-data,\n",
              ".xr-array-preview {\n",
              "  display: inline-block;\n",
              "}\n",
              "\n",
              ".xr-dim-list {\n",
              "  display: inline-block !important;\n",
              "  list-style: none;\n",
              "  padding: 0 !important;\n",
              "  margin: 0;\n",
              "}\n",
              "\n",
              ".xr-dim-list li {\n",
              "  display: inline-block;\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "}\n",
              "\n",
              ".xr-dim-list:before {\n",
              "  content: '(';\n",
              "}\n",
              "\n",
              ".xr-dim-list:after {\n",
              "  content: ')';\n",
              "}\n",
              "\n",
              ".xr-dim-list li:not(:last-child):after {\n",
              "  content: ',';\n",
              "  padding-right: 5px;\n",
              "}\n",
              "\n",
              ".xr-has-index {\n",
              "  font-weight: bold;\n",
              "}\n",
              "\n",
              ".xr-var-list,\n",
              ".xr-var-item {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-var-item > div,\n",
              ".xr-var-item label,\n",
              ".xr-var-item > .xr-var-name span {\n",
              "  background-color: var(--xr-background-color-row-even);\n",
              "  margin-bottom: 0;\n",
              "}\n",
              "\n",
              ".xr-var-item > .xr-var-name:hover span {\n",
              "  padding-right: 5px;\n",
              "}\n",
              "\n",
              ".xr-var-list > li:nth-child(odd) > div,\n",
              ".xr-var-list > li:nth-child(odd) > label,\n",
              ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
              "  background-color: var(--xr-background-color-row-odd);\n",
              "}\n",
              "\n",
              ".xr-var-name {\n",
              "  grid-column: 1;\n",
              "}\n",
              "\n",
              ".xr-var-dims {\n",
              "  grid-column: 2;\n",
              "}\n",
              "\n",
              ".xr-var-dtype {\n",
              "  grid-column: 3;\n",
              "  text-align: right;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-var-preview {\n",
              "  grid-column: 4;\n",
              "}\n",
              "\n",
              ".xr-index-preview {\n",
              "  grid-column: 2 / 5;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-var-name,\n",
              ".xr-var-dims,\n",
              ".xr-var-dtype,\n",
              ".xr-preview,\n",
              ".xr-attrs dt {\n",
              "  white-space: nowrap;\n",
              "  overflow: hidden;\n",
              "  text-overflow: ellipsis;\n",
              "  padding-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-var-name:hover,\n",
              ".xr-var-dims:hover,\n",
              ".xr-var-dtype:hover,\n",
              ".xr-attrs dt:hover {\n",
              "  overflow: visible;\n",
              "  width: auto;\n",
              "  z-index: 1;\n",
              "}\n",
              "\n",
              ".xr-var-attrs,\n",
              ".xr-var-data,\n",
              ".xr-index-data {\n",
              "  display: none;\n",
              "  background-color: var(--xr-background-color) !important;\n",
              "  padding-bottom: 5px !important;\n",
              "}\n",
              "\n",
              ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
              ".xr-var-data-in:checked ~ .xr-var-data,\n",
              ".xr-index-data-in:checked ~ .xr-index-data {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              ".xr-var-data > table {\n",
              "  float: right;\n",
              "}\n",
              "\n",
              ".xr-var-name span,\n",
              ".xr-var-data,\n",
              ".xr-index-name div,\n",
              ".xr-index-data,\n",
              ".xr-attrs {\n",
              "  padding-left: 25px !important;\n",
              "}\n",
              "\n",
              ".xr-attrs,\n",
              ".xr-var-attrs,\n",
              ".xr-var-data,\n",
              ".xr-index-data {\n",
              "  grid-column: 1 / -1;\n",
              "}\n",
              "\n",
              "dl.xr-attrs {\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "  display: grid;\n",
              "  grid-template-columns: 125px auto;\n",
              "}\n",
              "\n",
              ".xr-attrs dt,\n",
              ".xr-attrs dd {\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "  float: left;\n",
              "  padding-right: 10px;\n",
              "  width: auto;\n",
              "}\n",
              "\n",
              ".xr-attrs dt {\n",
              "  font-weight: normal;\n",
              "  grid-column: 1;\n",
              "}\n",
              "\n",
              ".xr-attrs dt:hover span {\n",
              "  display: inline-block;\n",
              "  background: var(--xr-background-color);\n",
              "  padding-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-attrs dd {\n",
              "  grid-column: 2;\n",
              "  white-space: pre-wrap;\n",
              "  word-break: break-all;\n",
              "}\n",
              "\n",
              ".xr-icon-database,\n",
              ".xr-icon-file-text2,\n",
              ".xr-no-icon {\n",
              "  display: inline-block;\n",
              "  vertical-align: middle;\n",
              "  width: 1em;\n",
              "  height: 1.5em !important;\n",
              "  stroke-width: 0;\n",
              "  stroke: currentColor;\n",
              "  fill: currentColor;\n",
              "}\n",
              "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray (time: 10, lat: 256, lon: 512)&gt;\n",
              "array([[[0.13664545, 0.32887597, 0.16400457, ..., 0.78938287,\n",
              "         0.24562398, 0.48045371],\n",
              "        [0.66315231, 0.70478573, 0.73073083, ..., 0.75658078,\n",
              "         0.74332593, 0.13430482],\n",
              "        [0.7173843 , 0.77349078, 0.67879202, ..., 0.31228275,\n",
              "         0.04309166, 0.61334845],\n",
              "        ...,\n",
              "        [0.06139518, 0.31531274, 0.42670884, ..., 0.93641654,\n",
              "         0.1011765 , 0.59698909],\n",
              "        [0.30094224, 0.09741645, 0.51358164, ..., 0.89605468,\n",
              "         0.42517612, 0.1787893 ],\n",
              "        [0.33244633, 0.5506997 , 0.46219064, ..., 0.17560451,\n",
              "         0.77574484, 0.89141205]],\n",
              "\n",
              "       [[0.35372644, 0.58949174, 0.33371672, ..., 0.23841651,\n",
              "         0.4197214 , 0.0418784 ],\n",
              "        [0.77661124, 0.14170365, 0.56176736, ..., 0.40929684,\n",
              "         0.27586892, 0.46166271],\n",
              "        [0.44123109, 0.08398815, 0.25544012, ..., 0.40527196,\n",
              "         0.04522792, 0.18044354],\n",
              "...\n",
              "        [0.00550026, 0.27413701, 0.56717994, ..., 0.83236285,\n",
              "         0.25179192, 0.19902724],\n",
              "        [0.4871888 , 0.86833526, 0.55141899, ..., 0.78331999,\n",
              "         0.42362269, 0.90254593],\n",
              "        [0.39569856, 0.57417873, 0.76809184, ..., 0.74587134,\n",
              "         0.13068811, 0.25544508]],\n",
              "\n",
              "       [[0.9470806 , 0.98569814, 0.16229052, ..., 0.37357493,\n",
              "         0.93713055, 0.82920746],\n",
              "        [0.69256943, 0.04596307, 0.70782498, ..., 0.15126597,\n",
              "         0.74277918, 0.34218779],\n",
              "        [0.58955714, 0.09528321, 0.16638967, ..., 0.88429185,\n",
              "         0.78529235, 0.38035069],\n",
              "        ...,\n",
              "        [0.13426855, 0.32683767, 0.92079097, ..., 0.07083984,\n",
              "         0.45406553, 0.55651957],\n",
              "        [0.3212409 , 0.89754804, 0.50476702, ..., 0.20709826,\n",
              "         0.35124961, 0.30008607],\n",
              "        [0.67135585, 0.05002323, 0.81442564, ..., 0.60543809,\n",
              "         0.18766374, 0.41956721]]])\n",
              "Dimensions without coordinates: time, lat, lon</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'></div><ul class='xr-dim-list'><li><span>time</span>: 10</li><li><span>lat</span>: 256</li><li><span>lon</span>: 512</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-04872d5a-b62d-4362-b7e8-ab74d11fa59c' class='xr-array-in' type='checkbox' checked><label for='section-04872d5a-b62d-4362-b7e8-ab74d11fa59c' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>0.1366 0.3289 0.164 0.8849 0.3017 ... 0.932 0.6054 0.1877 0.4196</span></div><div class='xr-array-data'><pre>array([[[0.13664545, 0.32887597, 0.16400457, ..., 0.78938287,\n",
              "         0.24562398, 0.48045371],\n",
              "        [0.66315231, 0.70478573, 0.73073083, ..., 0.75658078,\n",
              "         0.74332593, 0.13430482],\n",
              "        [0.7173843 , 0.77349078, 0.67879202, ..., 0.31228275,\n",
              "         0.04309166, 0.61334845],\n",
              "        ...,\n",
              "        [0.06139518, 0.31531274, 0.42670884, ..., 0.93641654,\n",
              "         0.1011765 , 0.59698909],\n",
              "        [0.30094224, 0.09741645, 0.51358164, ..., 0.89605468,\n",
              "         0.42517612, 0.1787893 ],\n",
              "        [0.33244633, 0.5506997 , 0.46219064, ..., 0.17560451,\n",
              "         0.77574484, 0.89141205]],\n",
              "\n",
              "       [[0.35372644, 0.58949174, 0.33371672, ..., 0.23841651,\n",
              "         0.4197214 , 0.0418784 ],\n",
              "        [0.77661124, 0.14170365, 0.56176736, ..., 0.40929684,\n",
              "         0.27586892, 0.46166271],\n",
              "        [0.44123109, 0.08398815, 0.25544012, ..., 0.40527196,\n",
              "         0.04522792, 0.18044354],\n",
              "...\n",
              "        [0.00550026, 0.27413701, 0.56717994, ..., 0.83236285,\n",
              "         0.25179192, 0.19902724],\n",
              "        [0.4871888 , 0.86833526, 0.55141899, ..., 0.78331999,\n",
              "         0.42362269, 0.90254593],\n",
              "        [0.39569856, 0.57417873, 0.76809184, ..., 0.74587134,\n",
              "         0.13068811, 0.25544508]],\n",
              "\n",
              "       [[0.9470806 , 0.98569814, 0.16229052, ..., 0.37357493,\n",
              "         0.93713055, 0.82920746],\n",
              "        [0.69256943, 0.04596307, 0.70782498, ..., 0.15126597,\n",
              "         0.74277918, 0.34218779],\n",
              "        [0.58955714, 0.09528321, 0.16638967, ..., 0.88429185,\n",
              "         0.78529235, 0.38035069],\n",
              "        ...,\n",
              "        [0.13426855, 0.32683767, 0.92079097, ..., 0.07083984,\n",
              "         0.45406553, 0.55651957],\n",
              "        [0.3212409 , 0.89754804, 0.50476702, ..., 0.20709826,\n",
              "         0.35124961, 0.30008607],\n",
              "        [0.67135585, 0.05002323, 0.81442564, ..., 0.60543809,\n",
              "         0.18766374, 0.41956721]]])</pre></div></div></li><li class='xr-section-item'><input id='section-cadc91f5-6489-4a7d-be7b-fa0bf1842502' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-cadc91f5-6489-4a7d-be7b-fa0bf1842502' class='xr-section-summary'  title='Expand/collapse section'>Coordinates: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-44401121-f050-4e53-b7b5-b3e221a74502' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-44401121-f050-4e53-b7b5-b3e221a74502' class='xr-section-summary'  title='Expand/collapse section'>Indexes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-74577790-f9a8-4780-bf52-0e204c938a8e' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-74577790-f9a8-4780-bf52-0e204c938a8e' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# complex activation functions\n",
        "\n",
        "class ComplexCardioid(nn.Module):\n",
        "    \"\"\"\n",
        "    Complex Cardioid activation function\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(ComplexCardioid, self).__init__()\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        out = 0.5 * (1. + torch.cos(z.angle())) * z\n",
        "        return out\n",
        "\n",
        "class ComplexReLU(nn.Module):\n",
        "    \"\"\"\n",
        "    Complex-valued variants of the ReLU activation function\n",
        "    \"\"\"\n",
        "    def __init__(self, negative_slope=0., mode=\"real\", bias_shape=None, scale=1.):\n",
        "        super(ComplexReLU, self).__init__()\n",
        "\n",
        "        # store parameters\n",
        "        self.mode = mode\n",
        "        if self.mode in [\"modulus\", \"halfplane\"]:\n",
        "            if bias_shape is not None:\n",
        "                self.bias = nn.Parameter(scale * torch.ones(bias_shape, dtype=torch.float32))\n",
        "            else:\n",
        "                self.bias = nn.Parameter(scale * torch.ones((1), dtype=torch.float32))\n",
        "        else:\n",
        "            self.bias = 0\n",
        "\n",
        "        self.negative_slope = negative_slope\n",
        "        self.act = nn.LeakyReLU(negative_slope = negative_slope)\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        if self.mode == \"cartesian\":\n",
        "            zr = torch.view_as_real(z)\n",
        "            za = self.act(zr)\n",
        "            out = torch.view_as_complex(za)\n",
        "\n",
        "        elif self.mode == \"modulus\":\n",
        "            zabs = torch.sqrt(torch.square(z.real) + torch.square(z.imag))\n",
        "            out = torch.where(zabs + self.bias > 0, (zabs + self.bias) * z / zabs, 0.0)\n",
        "\n",
        "        elif self.mode == \"cardioid\":\n",
        "            out = 0.5 * (1. + torch.cos(z.angle())) * z\n",
        "\n",
        "        # elif self.mode == \"halfplane\":\n",
        "        #     # bias is an angle parameter in this case\n",
        "        #     modified_angle = torch.angle(z) - self.bias\n",
        "        #     condition = torch.logical_and( (0. <= modified_angle), (modified_angle < torch.pi/2.) )\n",
        "        #     out = torch.where(condition, z, self.negative_slope * z)\n",
        "\n",
        "        elif self.mode == \"real\":\n",
        "            zr = torch.view_as_real(z)\n",
        "            outr = zr.clone()\n",
        "            outr[..., 0] = self.act(zr[..., 0])\n",
        "            out = torch.view_as_complex(outr)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "67WGUIW1jSyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\"\"\"\n",
        "Contains complex contractions wrapped into jit for harmonic layers\n",
        "\"\"\"\n",
        "\n",
        "@torch.jit.script\n",
        "def contract_diagonal(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    ac = torch.view_as_complex(a)\n",
        "    bc = torch.view_as_complex(b)\n",
        "    res = torch.einsum(\"bixy,kixy->bkxy\", ac, bc)\n",
        "    return torch.view_as_real(res)\n",
        "\n",
        "@torch.jit.script\n",
        "def contract_dhconv(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    ac = torch.view_as_complex(a)\n",
        "    bc = torch.view_as_complex(b)\n",
        "    res = torch.einsum(\"bixy,kix->bkxy\", ac, bc)\n",
        "    return torch.view_as_real(res)\n",
        "\n",
        "@torch.jit.script\n",
        "def contract_blockdiag(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    ac = torch.view_as_complex(a)\n",
        "    bc = torch.view_as_complex(b)\n",
        "    res = torch.einsum(\"bixy,kixyz->bkxz\", ac, bc)\n",
        "    return torch.view_as_real(res)\n",
        "\n",
        "# Helper routines for the non-linear FNOs (Attention-like)\n",
        "@torch.jit.script\n",
        "def compl_mul1d_fwd(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    tmp = torch.einsum(\"bixs,ior->srbox\", a, b)\n",
        "    res = torch.stack([tmp[0,0,...] - tmp[1,1,...], tmp[1,0,...] + tmp[0,1,...]], dim=-1)\n",
        "    return res\n",
        "\n",
        "@torch.jit.script\n",
        "def compl_mul1d_fwd_c(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    ac = torch.view_as_complex(a)\n",
        "    bc = torch.view_as_complex(b)\n",
        "    resc = torch.einsum(\"bix,io->box\", ac, bc)\n",
        "    res = torch.view_as_real(resc)\n",
        "    return res\n",
        "\n",
        "@torch.jit.script\n",
        "def compl_muladd1d_fwd(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
        "    res = compl_mul1d_fwd(a, b) + c\n",
        "    return res\n",
        "\n",
        "@torch.jit.script\n",
        "def compl_muladd1d_fwd_c(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
        "    tmpcc = torch.view_as_complex(compl_mul1d_fwd_c(a, b))\n",
        "    cc = torch.view_as_complex(c)\n",
        "    return torch.view_as_real(tmpcc + cc)\n",
        "\n",
        "# Helper routines for FFT MLPs\n",
        "\n",
        "@torch.jit.script\n",
        "def compl_mul2d_fwd(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    tmp = torch.einsum(\"bixys,ior->srboxy\", a, b)\n",
        "    res = torch.stack([tmp[0,0,...] - tmp[1,1,...], tmp[1,0,...] + tmp[0,1,...]], dim=-1)\n",
        "    return res\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def compl_mul2d_fwd_c(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    ac = torch.view_as_complex(a)\n",
        "    bc = torch.view_as_complex(b)\n",
        "    resc = torch.einsum(\"bixy,io->boxy\", ac, bc)\n",
        "    res = torch.view_as_real(resc)\n",
        "    return res\n",
        "\n",
        "@torch.jit.script\n",
        "def compl_muladd2d_fwd(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
        "    res = compl_mul2d_fwd(a, b) + c\n",
        "    return res\n",
        "\n",
        "@torch.jit.script\n",
        "def compl_muladd2d_fwd_c(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
        "    tmpcc = torch.view_as_complex(compl_mul2d_fwd_c(a, b))\n",
        "    cc = torch.view_as_complex(c)\n",
        "    return torch.view_as_real(tmpcc + cc)\n",
        "\n",
        "@torch.jit.script\n",
        "def real_mul2d_fwd(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    out = torch.einsum(\"bixy,io->boxy\", a, b)\n",
        "    return out\n",
        "\n",
        "@torch.jit.script\n",
        "def real_muladd2d_fwd(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
        "    return compl_mul2d_fwd_c(a, b) + c"
      ],
      "metadata": {
        "id": "n6o1ONi0kRzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import tensorly as tl\n",
        "tl.set_backend('pytorch')\n",
        "\n",
        "from tltorch.factorized_tensors.core import FactorizedTensor\n",
        "\n",
        "einsum_symbols = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "\n",
        "def _contract_dense(x, weight, separable=False, operator_type='diagonal'):\n",
        "    order = tl.ndim(x)\n",
        "    # batch-size, in_channels, x, y...\n",
        "    x_syms = list(einsum_symbols[:order])\n",
        "\n",
        "    # in_channels, out_channels, x, y...\n",
        "    weight_syms = list(x_syms[1:]) # no batch-size\n",
        "\n",
        "    # batch-size, out_channels, x, y...\n",
        "    if separable:\n",
        "        out_syms = [x_syms[0]] + list(weight_syms)\n",
        "    else:\n",
        "        weight_syms.insert(1, einsum_symbols[order]) # outputs\n",
        "        out_syms = list(weight_syms)\n",
        "        out_syms[0] = x_syms[0]\n",
        "\n",
        "    if operator_type == 'diagonal':\n",
        "        pass\n",
        "    elif operator_type == 'block-diagonal':\n",
        "        weight_syms.insert(-1, einsum_symbols[order+1])\n",
        "        out_syms[-1] = weight_syms[-2]\n",
        "    elif operator_type == 'driscoll-healy':\n",
        "        weight_syms.pop()\n",
        "    else:\n",
        "        raise ValueError(f\"Unkonw operator type {operator_type}\")\n",
        "\n",
        "    eq= ''.join(x_syms) + ',' + ''.join(weight_syms) + '->' + ''.join(out_syms)\n",
        "\n",
        "    if not torch.is_tensor(weight):\n",
        "        weight = weight.to_tensor()\n",
        "\n",
        "    return tl.einsum(eq, x, weight)\n",
        "\n",
        "def _contract_cp(x, cp_weight, separable=False, operator_type='diagonal'):\n",
        "    order = tl.ndim(x)\n",
        "\n",
        "    x_syms = str(einsum_symbols[:order])\n",
        "    rank_sym = einsum_symbols[order]\n",
        "    out_sym = einsum_symbols[order+1]\n",
        "    out_syms = list(x_syms)\n",
        "\n",
        "    if separable:\n",
        "        factor_syms = [einsum_symbols[1]+rank_sym] #in only\n",
        "    else:\n",
        "        out_syms[1] = out_sym\n",
        "        factor_syms = [einsum_symbols[1]+rank_sym, out_sym+rank_sym] #in, out\n",
        "\n",
        "    factor_syms += [xs+rank_sym for xs in x_syms[2:]] #x, y, ...\n",
        "\n",
        "    if operator_type == 'diagonal':\n",
        "        pass\n",
        "    elif operator_type == 'block-diagonal':\n",
        "        out_syms[-1] = einsum_symbols[order+2]\n",
        "        factor_syms += [out_syms[-1] + rank_sym]\n",
        "    elif operator_type == 'driscoll-healy':\n",
        "        factor_syms.pop()\n",
        "    else:\n",
        "        raise ValueError(f\"Unkonw operator type {operator_type}\")\n",
        "\n",
        "    eq = x_syms + ',' + rank_sym + ',' + ','.join(factor_syms) + '->' + ''.join(out_syms)\n",
        "\n",
        "    return tl.einsum(eq, x, cp_weight.weights, *cp_weight.factors)\n",
        "\n",
        "\n",
        "def _contract_tucker(x, tucker_weight, separable=False, operator_type='diagonal'):\n",
        "    order = tl.ndim(x)\n",
        "\n",
        "    x_syms = str(einsum_symbols[:order])\n",
        "    out_sym = einsum_symbols[order]\n",
        "    out_syms = list(x_syms)\n",
        "    if separable:\n",
        "        core_syms = einsum_symbols[order+1:2*order]\n",
        "        # factor_syms = [einsum_symbols[1]+core_syms[0]] #in only\n",
        "        factor_syms = [xs+rs for (xs, rs) in zip(x_syms[1:], core_syms)] #x, y, ...\n",
        "\n",
        "    else:\n",
        "        core_syms = einsum_symbols[order+1:2*order+1]\n",
        "        out_syms[1] = out_sym\n",
        "        factor_syms = [einsum_symbols[1]+core_syms[0], out_sym+core_syms[1]] #out, in\n",
        "        factor_syms += [xs+rs for (xs, rs) in zip(x_syms[2:], core_syms[2:])] #x, y, ...\n",
        "\n",
        "    if operator_type == 'diagonal':\n",
        "        pass\n",
        "    elif operator_type == 'block-diagonal':\n",
        "        raise NotImplementedError(f\"Operator type {operator_type} not implemented for Tucker\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unkonw operator type {operator_type}\")\n",
        "\n",
        "    eq = x_syms + ',' + core_syms + ',' + ','.join(factor_syms) + '->' + ''.join(out_syms)\n",
        "\n",
        "    return tl.einsum(eq, x, tucker_weight.core, *tucker_weight.factors)\n",
        "\n",
        "def _contract_tt(x, tt_weight, separable=False, operator_type='diagonal'):\n",
        "    order = tl.ndim(x)\n",
        "\n",
        "    x_syms = list(einsum_symbols[:order])\n",
        "    weight_syms = list(x_syms[1:]) # no batch-size\n",
        "\n",
        "    if not separable:\n",
        "        weight_syms.insert(1, einsum_symbols[order]) # outputs\n",
        "        out_syms = list(weight_syms)\n",
        "        out_syms[0] = x_syms[0]\n",
        "    else:\n",
        "        out_syms = list(x_syms)\n",
        "\n",
        "    if operator_type == 'diagonal':\n",
        "        pass\n",
        "    elif operator_type == 'block-diagonal':\n",
        "        weight_syms.insert(-1, einsum_symbols[order+1])\n",
        "        out_syms[-1] = weight_syms[-2]\n",
        "    elif operator_type == 'driscoll-healy':\n",
        "        weight_syms.pop()\n",
        "    else:\n",
        "        raise ValueError(f\"Unkonw operator type {operator_type}\")\n",
        "\n",
        "    rank_syms = list(einsum_symbols[order+2:])\n",
        "    tt_syms = []\n",
        "    for i, s in enumerate(weight_syms):\n",
        "        tt_syms.append([rank_syms[i], s, rank_syms[i+1]])\n",
        "    eq = ''.join(x_syms) + ',' + ','.join(''.join(f) for f in tt_syms) + '->' + ''.join(out_syms)\n",
        "\n",
        "    return tl.einsum(eq, x, *tt_weight.factors)\n",
        "\n",
        "\n",
        "def get_contract_fun(weight, implementation='reconstructed', separable=False):\n",
        "    \"\"\"Generic ND implementation of Fourier Spectral Conv contraction\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    weight : tensorly-torch's FactorizedTensor\n",
        "    implementation : {'reconstructed', 'factorized'}, default is 'reconstructed'\n",
        "        whether to reconstruct the weight and do a forward pass (reconstructed)\n",
        "        or contract directly the factors of the factorized weight with the input (factorized)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    function : (x, weight) -> x * weight in Fourier space\n",
        "    \"\"\"\n",
        "    if implementation == 'reconstructed':\n",
        "        return _contract_dense\n",
        "    elif implementation == 'factorized':\n",
        "        if torch.is_tensor(weight):\n",
        "            return _contract_dense\n",
        "        elif isinstance(weight, FactorizedTensor):\n",
        "            if weight.name.lower() == 'complexdense':\n",
        "                return _contract_dense\n",
        "            elif weight.name.lower() == 'complextucker':\n",
        "                return _contract_tucker\n",
        "            elif weight.name.lower() == 'complextt':\n",
        "                return _contract_tt\n",
        "            elif weight.name.lower() == 'complexcp':\n",
        "                return _contract_cp\n",
        "            else:\n",
        "                raise ValueError(f'Got unexpected factorized weight type {weight.name}')\n",
        "        else:\n",
        "            raise ValueError(f'Got unexpected weight type of class {weight.__class__.__name__}')\n",
        "    else:\n",
        "        raise ValueError(f'Got {implementation=}, expected \"reconstructed\" or \"factorized\"')"
      ],
      "metadata": {
        "id": "R0ocpGyEkzfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.fft\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.cuda import amp\n",
        "import math\n",
        "\n",
        "# # import FactorizedTensor from tensorly for tensorized operations\n",
        "# import tensorly as tl\n",
        "# from tensorly.plugins import use_opt_einsum\n",
        "# tl.set_backend(\"pytorch\")\n",
        "# use_opt_einsum(\"optimal\")\n",
        "from tltorch.factorized_tensors.core import FactorizedTensor\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
        "    normal distribution. The values are effectively drawn from the\n",
        "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
        "    with values outside :math:`[a, b]` redrawn until they are within\n",
        "    the bounds. The method used for generating the random values works\n",
        "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
        "    Args:\n",
        "    tensor: an n-dimensional `torch.Tensor`\n",
        "    mean: the mean of the normal distribution\n",
        "    std: the standard deviation of the normal distribution\n",
        "    a: the minimum cutoff value\n",
        "    b: the maximum cutoff value\n",
        "    Examples:\n",
        "    >>> w = torch.empty(3, 5)\n",
        "    >>> nn.init.trunc_normal_(w)\n",
        "    \"\"\"\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def drop_path(x: torch.Tensor, drop_prob: float = 0., training: bool = False) -> torch.Tensor:\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1. - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2d ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 hidden_features = None,\n",
        "                 out_features = None,\n",
        "                 act_layer = nn.ReLU,\n",
        "                 output_bias = False,\n",
        "                 drop_rate = 0.,\n",
        "                 checkpointing = False,\n",
        "                 gain = 1.0):\n",
        "        super(MLP, self).__init__()\n",
        "        self.checkpointing = checkpointing\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "\n",
        "        # Fist dense layer\n",
        "        fc1 = nn.Conv2d(in_features, hidden_features, 1, bias=True)\n",
        "        # initialize the weights correctly\n",
        "        scale = math.sqrt(2.0 / in_features)\n",
        "        nn.init.normal_(fc1.weight, mean=0., std=scale)\n",
        "        if fc1.bias is not None:\n",
        "            nn.init.constant_(fc1.bias, 0.0)\n",
        "\n",
        "        # activation\n",
        "        act = act_layer()\n",
        "\n",
        "        # output layer\n",
        "        fc2 = nn.Conv2d(hidden_features, out_features, 1, bias=output_bias)\n",
        "        # gain factor for the output determines the scaling of the output init\n",
        "        scale = math.sqrt(gain / hidden_features)\n",
        "        nn.init.normal_(fc2.weight, mean=0., std=scale)\n",
        "        if fc2.bias is not None:\n",
        "            nn.init.constant_(fc2.bias, 0.0)\n",
        "\n",
        "        if drop_rate > 0.:\n",
        "            drop = nn.Dropout2d(drop_rate)\n",
        "            self.fwd = nn.Sequential(fc1, act, drop, fc2, drop)\n",
        "        else:\n",
        "            self.fwd = nn.Sequential(fc1, act, fc2)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def checkpoint_forward(self, x):\n",
        "        return checkpoint(self.fwd, x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.checkpointing:\n",
        "            return self.checkpoint_forward(x)\n",
        "        else:\n",
        "            return self.fwd(x)\n",
        "\n",
        "class RealFFT2(nn.Module):\n",
        "    \"\"\"\n",
        "    Helper routine to wrap FFT similarly to the SHT\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 nlat,\n",
        "                 nlon,\n",
        "                 lmax = None,\n",
        "                 mmax = None):\n",
        "        super(RealFFT2, self).__init__()\n",
        "\n",
        "        self.nlat = nlat\n",
        "        self.nlon = nlon\n",
        "        self.lmax = lmax or self.nlat\n",
        "        self.mmax = mmax or self.nlon // 2 + 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = torch.fft.rfft2(x, dim=(-2, -1), norm=\"ortho\")\n",
        "        y = torch.cat((y[..., :math.ceil(self.lmax/2), :self.mmax], y[..., -math.floor(self.lmax/2):, :self.mmax]), dim=-2)\n",
        "        return y\n",
        "\n",
        "class InverseRealFFT2(nn.Module):\n",
        "    \"\"\"\n",
        "    Helper routine to wrap FFT similarly to the SHT\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 nlat,\n",
        "                 nlon,\n",
        "                 lmax = None,\n",
        "                 mmax = None):\n",
        "        super(InverseRealFFT2, self).__init__()\n",
        "\n",
        "        self.nlat = nlat\n",
        "        self.nlon = nlon\n",
        "        self.lmax = lmax or self.nlat\n",
        "        self.mmax = mmax or self.nlon // 2 + 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.fft.irfft2(x, dim=(-2, -1), s=(self.nlat, self.nlon), norm=\"ortho\")\n",
        "\n",
        "class SpectralConvS2(nn.Module):\n",
        "    \"\"\"\n",
        "    Spectral Convolution according to Driscoll & Healy. Designed for convolutions on the two-sphere S2\n",
        "    using the Spherical Harmonic Transforms in torch-harmonics, but supports convolutions on the periodic\n",
        "    domain via the RealFFT2 and InverseRealFFT2 wrappers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 forward_transform,\n",
        "                 inverse_transform,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 gain = 2.,\n",
        "                 operator_type = \"driscoll-healy\",\n",
        "                 lr_scale_exponent = 0,\n",
        "                 bias = False):\n",
        "        super(SpectralConvS2, self).__init__()\n",
        "\n",
        "        self.forward_transform = forward_transform\n",
        "        self.inverse_transform = inverse_transform\n",
        "\n",
        "        self.modes_lat = self.inverse_transform.lmax\n",
        "        self.modes_lon = self.inverse_transform.mmax\n",
        "\n",
        "        self.scale_residual = (self.forward_transform.nlat != self.inverse_transform.nlat) \\\n",
        "                        or (self.forward_transform.nlon != self.inverse_transform.nlon)\n",
        "\n",
        "        # remember factorization details\n",
        "        self.operator_type = operator_type\n",
        "\n",
        "        assert self.inverse_transform.lmax == self.modes_lat\n",
        "        assert self.inverse_transform.mmax == self.modes_lon\n",
        "\n",
        "        weight_shape = [out_channels, in_channels]\n",
        "\n",
        "        if self.operator_type == \"diagonal\":\n",
        "            weight_shape += [self.modes_lat, self.modes_lon]\n",
        "            _contract = contract_diagonal\n",
        "        elif self.operator_type == \"block-diagonal\":\n",
        "            weight_shape += [self.modes_lat, self.modes_lon, self.modes_lon]\n",
        "            _contract = contract_blockdiag\n",
        "        elif self.operator_type == \"driscoll-healy\":\n",
        "            weight_shape += [self.modes_lat]\n",
        "            _contract = contract_dhconv\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unkonw operator type f{self.operator_type}\")\n",
        "\n",
        "        # form weight tensors\n",
        "        scale = math.sqrt(gain / in_channels) * torch.ones(self.modes_lat, 2)\n",
        "        scale[0] *=  math.sqrt(2)\n",
        "        self.weight = nn.Parameter(scale * torch.view_as_real(torch.randn(*weight_shape, dtype=torch.complex64)))\n",
        "        # self.weight = nn.Parameter(scale * torch.randn(*weight_shape, 2))\n",
        "\n",
        "        # get the right contraction function\n",
        "        self._contract = _contract\n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(1, out_channels, 1, 1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        dtype = x.dtype\n",
        "        x = x.float()\n",
        "        residual = x\n",
        "\n",
        "        with amp.autocast(enabled=False):\n",
        "            x = self.forward_transform(x)\n",
        "            if self.scale_residual:\n",
        "                residual = self.inverse_transform(x)\n",
        "\n",
        "\n",
        "        x = torch.view_as_real(x)\n",
        "        x = self._contract(x, self.weight)\n",
        "        x = torch.view_as_complex(x)\n",
        "\n",
        "        with amp.autocast(enabled=False):\n",
        "            x = self.inverse_transform(x)\n",
        "\n",
        "        if hasattr(self, \"bias\"):\n",
        "            x = x + self.bias\n",
        "        x = x.type(dtype)\n",
        "\n",
        "        return x, residual\n",
        "\n",
        "class FactorizedSpectralConvS2(nn.Module):\n",
        "    \"\"\"\n",
        "    Factorized version of SpectralConvS2. Uses tensorly-torch to keep the weights factorized\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 forward_transform,\n",
        "                 inverse_transform,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 gain = 2.,\n",
        "                 operator_type = \"driscoll-healy\",\n",
        "                 rank = 0.2,\n",
        "                 factorization = None,\n",
        "                 separable = False,\n",
        "                 implementation = \"factorized\",\n",
        "                 decomposition_kwargs=dict(),\n",
        "                 bias = False):\n",
        "        super(SpectralConvS2, self).__init__()\n",
        "\n",
        "        self.forward_transform = forward_transform\n",
        "        self.inverse_transform = inverse_transform\n",
        "\n",
        "        self.modes_lat = self.inverse_transform.lmax\n",
        "        self.modes_lon = self.inverse_transform.mmax\n",
        "\n",
        "        self.scale_residual = (self.forward_transform.nlat != self.inverse_transform.nlat) \\\n",
        "                        or (self.forward_transform.nlon != self.inverse_transform.nlon)\n",
        "\n",
        "        # Make sure we are using a Complex Factorized Tensor\n",
        "        if factorization is None:\n",
        "            factorization = \"Dense\" # No factorization\n",
        "        if not factorization.lower().startswith(\"complex\"):\n",
        "            factorization = f\"Complex{factorization}\"\n",
        "\n",
        "        # remember factorization details\n",
        "        self.operator_type = operator_type\n",
        "        self.rank = rank\n",
        "        self.factorization = factorization\n",
        "        self.separable = separable\n",
        "\n",
        "        assert self.inverse_transform.lmax == self.modes_lat\n",
        "        assert self.inverse_transform.mmax == self.modes_lon\n",
        "\n",
        "        weight_shape = [out_channels]\n",
        "\n",
        "        if not self.separable:\n",
        "            weight_shape += [in_channels]\n",
        "\n",
        "        if self.operator_type == \"diagonal\":\n",
        "            weight_shape += [self.modes_lat, self.modes_lon]\n",
        "        elif self.operator_type == \"block-diagonal\":\n",
        "            weight_shape += [self.modes_lat, self.modes_lon, self.modes_lon]\n",
        "        elif self.operator_type == \"driscoll-healy\":\n",
        "            weight_shape += [self.modes_lat]\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unkonw operator type f{self.operator_type}\")\n",
        "\n",
        "        # form weight tensors\n",
        "        self.weight = FactorizedTensor.new(weight_shape, rank=self.rank, factorization=factorization,\n",
        "                                           fixed_rank_modes=False, **decomposition_kwargs)\n",
        "\n",
        "        # initialization of weights\n",
        "        scale = math.sqrt(gain / in_channels)\n",
        "        self.weight.normal_(0, scale)\n",
        "\n",
        "        # get the right contraction function\n",
        "        from .factorizations import get_contract_fun\n",
        "        self._contract = get_contract_fun(self.weight, implementation=implementation, separable=separable)\n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(1, out_channels, 1, 1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        dtype = x.dtype\n",
        "        x = x.float()\n",
        "        residual = x\n",
        "\n",
        "        with amp.autocast(enabled=False):\n",
        "            x = self.forward_transform(x)\n",
        "            if self.scale_residual:\n",
        "                residual = self.inverse_transform(x)\n",
        "\n",
        "        x = self._contract(x, self.weight, separable=self.separable, operator_type=self.operator_type)\n",
        "\n",
        "        with amp.autocast(enabled=False):\n",
        "            x = self.inverse_transform(x)\n",
        "\n",
        "        if hasattr(self, \"bias\"):\n",
        "            x = x + self.bias\n",
        "        x = x.type(dtype)\n",
        "\n",
        "        return x, residual"
      ],
      "metadata": {
        "id": "ahFAfZEKk3Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch_harmonics import *\n",
        "\n",
        "from functools import partial"
      ],
      "metadata": {
        "id": "d60YAtVFlZbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpectralFilterLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Fourier layer. Contains the convolution part of the FNO/SFNO\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        forward_transform,\n",
        "        inverse_transform,\n",
        "        input_dim,\n",
        "        output_dim,\n",
        "        gain = 2.,\n",
        "        operator_type = \"diagonal\",\n",
        "        hidden_size_factor = 2,\n",
        "        factorization = None,\n",
        "        separable = False,\n",
        "        rank = 1e-2,\n",
        "        bias = True):\n",
        "        super(SpectralFilterLayer, self).__init__()\n",
        "\n",
        "        if factorization is None:\n",
        "            self.filter = SpectralConvS2(forward_transform,\n",
        "                                         inverse_transform,\n",
        "                                         input_dim,\n",
        "                                         output_dim,\n",
        "                                         gain = gain,\n",
        "                                         operator_type = operator_type,\n",
        "                                         bias = bias)\n",
        "\n",
        "        elif factorization is not None:\n",
        "            self.filter = FactorizedSpectralConvS2(forward_transform,\n",
        "                                                   inverse_transform,\n",
        "                                                   input_dim,\n",
        "                                                   output_dim,\n",
        "                                                   gain = gain,\n",
        "                                                   operator_type = operator_type,\n",
        "                                                   rank = rank,\n",
        "                                                   factorization = factorization,\n",
        "                                                   separable = separable,\n",
        "                                                   bias = bias)\n",
        "\n",
        "        else:\n",
        "            raise(NotImplementedError)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.filter(x)\n",
        "\n",
        "class SphericalFourierNeuralOperatorBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Helper module for a single SFNO/FNO block. Can use both FFTs and SHTs to represent either FNO or SFNO blocks.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            forward_transform,\n",
        "            inverse_transform,\n",
        "            input_dim,\n",
        "            output_dim,\n",
        "            operator_type = \"driscoll-healy\",\n",
        "            mlp_ratio = 2.,\n",
        "            drop_rate = 0.,\n",
        "            drop_path = 0.,\n",
        "            act_layer = nn.ReLU,\n",
        "            norm_layer = nn.Identity,\n",
        "            factorization = None,\n",
        "            separable = False,\n",
        "            rank = 128,\n",
        "            inner_skip = \"linear\",\n",
        "            outer_skip = None,\n",
        "            use_mlp = True):\n",
        "        super(SphericalFourierNeuralOperatorBlock, self).__init__()\n",
        "\n",
        "        if act_layer == nn.Identity:\n",
        "            gain_factor = 1.0\n",
        "        else:\n",
        "            gain_factor = 2.0\n",
        "\n",
        "        if inner_skip == \"linear\" or inner_skip == \"identity\":\n",
        "            gain_factor /= 2.0\n",
        "\n",
        "        # convolution layer\n",
        "        self.filter = SpectralFilterLayer(forward_transform,\n",
        "                                          inverse_transform,\n",
        "                                          input_dim,\n",
        "                                          output_dim,\n",
        "                                          gain = gain_factor,\n",
        "                                          operator_type = operator_type,\n",
        "                                          hidden_size_factor = mlp_ratio,\n",
        "                                          factorization = factorization,\n",
        "                                          separable = separable,\n",
        "                                          rank = rank,\n",
        "                                          bias = True)\n",
        "\n",
        "        if inner_skip == \"linear\":\n",
        "            self.inner_skip = nn.Conv2d(input_dim, output_dim, 1, 1)\n",
        "            nn.init.normal_(self.inner_skip.weight, std=math.sqrt(gain_factor/input_dim))\n",
        "        elif inner_skip == \"identity\":\n",
        "            assert input_dim == output_dim\n",
        "            self.inner_skip = nn.Identity()\n",
        "        elif inner_skip == \"none\":\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown skip connection type {inner_skip}\")\n",
        "\n",
        "        self.act_layer = act_layer()\n",
        "\n",
        "        # first normalisation layer\n",
        "        self.norm0 = norm_layer()\n",
        "\n",
        "        # dropout\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "        gain_factor = 1.0\n",
        "        if outer_skip == \"linear\" or inner_skip == \"identity\":\n",
        "            gain_factor /= 2.\n",
        "\n",
        "        if use_mlp == True:\n",
        "            mlp_hidden_dim = int(output_dim * mlp_ratio)\n",
        "            self.mlp = MLP(in_features = output_dim,\n",
        "                           out_features = input_dim,\n",
        "                           hidden_features = mlp_hidden_dim,\n",
        "                           act_layer = act_layer,\n",
        "                           drop_rate = drop_rate,\n",
        "                           checkpointing = False,\n",
        "                           gain = gain_factor)\n",
        "\n",
        "        if outer_skip == \"linear\":\n",
        "            self.outer_skip = nn.Conv2d(input_dim, input_dim, 1, 1)\n",
        "            torch.nn.init.normal_(self.outer_skip.weight, std=math.sqrt(gain_factor/input_dim))\n",
        "        elif outer_skip == \"identity\":\n",
        "            assert input_dim == output_dim\n",
        "            self.outer_skip = nn.Identity()\n",
        "        elif outer_skip == \"none\":\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown skip connection type {outer_skip}\")\n",
        "\n",
        "        # second normalisation layer\n",
        "        self.norm1 = norm_layer()\n",
        "\n",
        "    # def init_weights(self, scale):\n",
        "    #     if hasattr(self, \"inner_skip\") and isinstance(self.inner_skip, nn.Conv2d):\n",
        "    #         gain_factor = 1.\n",
        "    #         scale = (gain_factor / embed_dim)**0.5\n",
        "    #         nn.init.normal_(self.inner_skip.weight, mean=0., std=scale)\n",
        "    #         self.filter.filter.init_weights(scale)\n",
        "    #     else:\n",
        "    #         gain_factor = 2.\n",
        "    #         scale = (gain_factor / embed_dim)**0.5\n",
        "    #         self.filter.filter.init_weights(scale)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x, residual = self.filter(x)\n",
        "\n",
        "        x = self.norm0(x)\n",
        "\n",
        "        if hasattr(self, \"inner_skip\"):\n",
        "            x = x + self.inner_skip(residual)\n",
        "\n",
        "        if hasattr(self, \"act_layer\"):\n",
        "            x = self.act_layer(x)\n",
        "\n",
        "        if hasattr(self, \"mlp\"):\n",
        "            x = self.mlp(x)\n",
        "\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        x = self.drop_path(x)\n",
        "\n",
        "        if hasattr(self, \"outer_skip\"):\n",
        "            x = x + self.outer_skip(residual)\n",
        "\n",
        "        return x\n",
        "\n",
        "class SphericalFourierNeuralOperatorNet(nn.Module):\n",
        "    \"\"\"\n",
        "    SphericalFourierNeuralOperator module. Can use both FFTs and SHTs to represent either FNO or SFNO,\n",
        "    both linear and non-linear variants.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    spectral_transform : str, optional\n",
        "        Type of spectral transformation to use, by default \"sht\"\n",
        "    operator_type : str, optional\n",
        "        Type of operator to use ('driscoll-healy', 'diagonal'), by default \"driscoll-healy\"\n",
        "    img_shape : tuple, optional\n",
        "        Shape of the input channels, by default (128, 256)\n",
        "    scale_factor : int, optional\n",
        "        Scale factor to use, by default 3\n",
        "    in_chans : int, optional\n",
        "        Number of input channels, by default 3\n",
        "    out_chans : int, optional\n",
        "        Number of output channels, by default 3\n",
        "    embed_dim : int, optional\n",
        "        Dimension of the embeddings, by default 256\n",
        "    num_layers : int, optional\n",
        "        Number of layers in the network, by default 4\n",
        "    activation_function : str, optional\n",
        "        Activation function to use, by default \"gelu\"\n",
        "    encoder_layers : int, optional\n",
        "        Number of layers in the encoder, by default 1\n",
        "    use_mlp : int, optional\n",
        "        Whether to use MLPs in the SFNO blocks, by default True\n",
        "    mlp_ratio : int, optional\n",
        "        Ratio of MLP to use, by default 2.0\n",
        "    drop_rate : float, optional\n",
        "        Dropout rate, by default 0.0\n",
        "    drop_path_rate : float, optional\n",
        "        Dropout path rate, by default 0.0\n",
        "    normalization_layer : str, optional\n",
        "        Type of normalization layer to use (\"layer_norm\", \"instance_norm\", \"none\"), by default \"instance_norm\"\n",
        "    hard_thresholding_fraction : float, optional\n",
        "        Fraction of hard thresholding (frequency cutoff) to apply, by default 1.0\n",
        "    big_skip : bool, optional\n",
        "        Whether to add a single large skip connection, by default True\n",
        "    rank : float, optional\n",
        "        Rank of the approximation, by default 1.0\n",
        "    factorization : Any, optional\n",
        "        Type of factorization to use, by default None\n",
        "    separable : bool, optional\n",
        "        Whether to use separable convolutions, by default False\n",
        "    rank : (int, Tuple[int]), optional\n",
        "        If a factorization is used, which rank to use. Argument is passed to tensorly\n",
        "    pos_embed : bool, optional\n",
        "        Whether to use positional embedding, by default True\n",
        "\n",
        "    Example:\n",
        "    --------\n",
        "    >>> model = SphericalFourierNeuralOperatorNet(\n",
        "    ...         img_shape=(128, 256),\n",
        "    ...         scale_factor=4,\n",
        "    ...         in_chans=2,\n",
        "    ...         out_chans=2,\n",
        "    ...         embed_dim=16,\n",
        "    ...         num_layers=4,\n",
        "    ...         use_mlp=True,)\n",
        "    >>> model(torch.randn(1, 2, 128, 256)).shape\n",
        "    torch.Size([1, 2, 128, 256])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            spectral_transform = \"sht\",\n",
        "            operator_type = \"driscoll-healy\",\n",
        "            img_size = (128, 256),\n",
        "            grid = \"equiangular\",\n",
        "            scale_factor = 3,\n",
        "            in_chans = 3,\n",
        "            out_chans = 3,\n",
        "            embed_dim = 256,\n",
        "            num_layers = 4,\n",
        "            activation_function = \"relu\",\n",
        "            encoder_layers = 1,\n",
        "            use_mlp = True,\n",
        "            mlp_ratio = 2.,\n",
        "            drop_rate = 0.,\n",
        "            drop_path_rate = 0.,\n",
        "            normalization_layer = \"none\",\n",
        "            hard_thresholding_fraction = 1.0,\n",
        "            use_complex_kernels = True,\n",
        "            big_skip = False,\n",
        "            factorization = None,\n",
        "            separable = False,\n",
        "            rank = 128,\n",
        "            pos_embed = False):\n",
        "\n",
        "        super(SphericalFourierNeuralOperatorNet, self).__init__()\n",
        "\n",
        "        self.spectral_transform = spectral_transform\n",
        "        self.operator_type = operator_type\n",
        "        self.img_size = img_size\n",
        "        self.grid = grid\n",
        "        self.scale_factor = scale_factor\n",
        "        self.in_chans = in_chans\n",
        "        self.out_chans = out_chans\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.hard_thresholding_fraction = hard_thresholding_fraction\n",
        "        self.normalization_layer = normalization_layer\n",
        "        self.use_mlp = use_mlp\n",
        "        self.encoder_layers = encoder_layers\n",
        "        self.big_skip = big_skip\n",
        "        self.factorization = factorization\n",
        "        self.separable = separable,\n",
        "        self.rank = rank\n",
        "\n",
        "        # activation function\n",
        "        if activation_function == \"relu\":\n",
        "            self.activation_function = nn.ReLU\n",
        "        elif activation_function == \"gelu\":\n",
        "            self.activation_function = nn.GELU\n",
        "        # for debugging purposes\n",
        "        elif activation_function == \"identity\":\n",
        "            self.activation_function = nn.Identity\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation function {activation_function}\")\n",
        "\n",
        "        # compute downsampled image size\n",
        "        self.h = self.img_size[0] // scale_factor\n",
        "        self.w = self.img_size[1] // scale_factor\n",
        "\n",
        "        # dropout\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate) if drop_rate > 0. else nn.Identity()\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, self.num_layers)]\n",
        "\n",
        "        # pick norm layer\n",
        "        if self.normalization_layer == \"layer_norm\":\n",
        "            norm_layer0 = partial(nn.LayerNorm, normalized_shape=(self.img_size[0], self.img_size[1]), eps=1e-6)\n",
        "            norm_layer1 = partial(nn.LayerNorm, normalized_shape=(self.h, self.w), eps=1e-6)\n",
        "        elif self.normalization_layer == \"instance_norm\":\n",
        "            norm_layer0 = partial(nn.InstanceNorm2d, num_features=self.embed_dim, eps=1e-6, affine=True, track_running_stats=False)\n",
        "            norm_layer1 = partial(nn.InstanceNorm2d, num_features=self.embed_dim, eps=1e-6, affine=True, track_running_stats=False)\n",
        "        elif self.normalization_layer == \"none\":\n",
        "            norm_layer0 = nn.Identity\n",
        "            norm_layer1 = norm_layer0\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Error, normalization {self.normalization_layer} not implemented.\")\n",
        "\n",
        "        if pos_embed == \"latlon\" or pos_embed==True:\n",
        "            self.pos_embed = nn.Parameter(torch.zeros(1, self.embed_dim, self.img_size[0], self.img_size[1]))\n",
        "            nn.init.constant_(self.pos_embed, 0.0)\n",
        "        elif pos_embed == \"lat\":\n",
        "            self.pos_embed = nn.Parameter(torch.zeros(1, self.embed_dim, self.img_size[0], 1))\n",
        "            nn.init.constant_(self.pos_embed, 0.0)\n",
        "        elif pos_embed == \"const\":\n",
        "            self.pos_embed = nn.Parameter(torch.zeros(1, self.embed_dim, 1, 1))\n",
        "            nn.init.constant_(self.pos_embed, 0.0)\n",
        "        else:\n",
        "            self.pos_embed = None\n",
        "\n",
        "        # # encoder\n",
        "        # encoder_hidden_dim = int(self.embed_dim * mlp_ratio)\n",
        "        # encoder = MLP(in_features = self.in_chans,\n",
        "        #               out_features = self.embed_dim,\n",
        "        #               hidden_features = encoder_hidden_dim,\n",
        "        #               act_layer = self.activation_function,\n",
        "        #               drop_rate = drop_rate,\n",
        "        #               checkpointing = False)\n",
        "        # self.encoder = encoder\n",
        "\n",
        "\n",
        "        # construct an encoder with num_encoder_layers\n",
        "        num_encoder_layers = 1\n",
        "        encoder_hidden_dim = int(self.embed_dim * mlp_ratio)\n",
        "        current_dim = self.in_chans\n",
        "        encoder_layers = []\n",
        "        for l in range(num_encoder_layers-1):\n",
        "            fc = nn.Conv2d(current_dim, encoder_hidden_dim, 1, bias=True)\n",
        "            # initialize the weights correctly\n",
        "            scale = math.sqrt(2. / current_dim)\n",
        "            nn.init.normal_(fc.weight, mean=0., std=scale)\n",
        "            if fc.bias is not None:\n",
        "                nn.init.constant_(fc.bias, 0.0)\n",
        "            encoder_layers.append(fc)\n",
        "            encoder_layers.append(self.activation_function())\n",
        "            current_dim = encoder_hidden_dim\n",
        "        fc = nn.Conv2d(current_dim, self.embed_dim, 1, bias=False)\n",
        "        scale = math.sqrt(1. / current_dim)\n",
        "        nn.init.normal_(fc.weight, mean=0., std=scale)\n",
        "        if fc.bias is not None:\n",
        "            nn.init.constant_(fc.bias, 0.0)\n",
        "        encoder_layers.append(fc)\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # prepare the spectral transform\n",
        "        if self.spectral_transform == \"sht\":\n",
        "\n",
        "            modes_lat = int(self.h * self.hard_thresholding_fraction)\n",
        "            modes_lon = int(self.w//2 * self.hard_thresholding_fraction)\n",
        "            modes_lat = modes_lon = min(modes_lat, modes_lon)\n",
        "\n",
        "            self.trans_down = RealSHT(*self.img_size, lmax=modes_lat, mmax=modes_lon, grid=self.grid).float()\n",
        "            self.itrans_up  = InverseRealSHT(*self.img_size, lmax=modes_lat, mmax=modes_lon, grid=self.grid).float()\n",
        "            self.trans      = RealSHT(self.h, self.w, lmax=modes_lat, mmax=modes_lon, grid=\"legendre-gauss\").float()\n",
        "            self.itrans     = InverseRealSHT(self.h, self.w, lmax=modes_lat, mmax=modes_lon, grid=\"legendre-gauss\").float()\n",
        "\n",
        "        elif self.spectral_transform == \"fft\":\n",
        "\n",
        "            modes_lat = int(self.h * self.hard_thresholding_fraction)\n",
        "            modes_lon = int((self.w // 2 + 1) * self.hard_thresholding_fraction)\n",
        "\n",
        "            self.trans_down = RealFFT2(*self.img_size, lmax=modes_lat, mmax=modes_lon).float()\n",
        "            self.itrans_up  = InverseRealFFT2(*self.img_size, lmax=modes_lat, mmax=modes_lon).float()\n",
        "            self.trans      = RealFFT2(self.h, self.w, lmax=modes_lat, mmax=modes_lon).float()\n",
        "            self.itrans     = InverseRealFFT2(self.h, self.w, lmax=modes_lat, mmax=modes_lon).float()\n",
        "\n",
        "        else:\n",
        "            raise(ValueError(\"Unknown spectral transform\"))\n",
        "\n",
        "        self.blocks = nn.ModuleList([])\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            first_layer = i == 0\n",
        "            last_layer = i == self.num_layers-1\n",
        "\n",
        "            forward_transform = self.trans_down if first_layer else self.trans\n",
        "            inverse_transform = self.itrans_up if last_layer else self.itrans\n",
        "\n",
        "            inner_skip = \"none\"\n",
        "            outer_skip = \"identity\"\n",
        "\n",
        "            if first_layer:\n",
        "                norm_layer = norm_layer1\n",
        "            elif last_layer:\n",
        "                norm_layer = norm_layer0\n",
        "            else:\n",
        "                norm_layer = norm_layer1\n",
        "\n",
        "            block = SphericalFourierNeuralOperatorBlock(forward_transform,\n",
        "                                                        inverse_transform,\n",
        "                                                        self.embed_dim,\n",
        "                                                        self.embed_dim,\n",
        "                                                        operator_type = self.operator_type,\n",
        "                                                        mlp_ratio = mlp_ratio,\n",
        "                                                        drop_rate = drop_rate,\n",
        "                                                        drop_path = dpr[i],\n",
        "                                                        act_layer = self.activation_function,\n",
        "                                                        norm_layer = norm_layer,\n",
        "                                                        inner_skip = inner_skip,\n",
        "                                                        outer_skip = outer_skip,\n",
        "                                                        use_mlp = use_mlp,\n",
        "                                                        factorization = self.factorization,\n",
        "                                                        separable = self.separable,\n",
        "                                                        rank = self.rank)\n",
        "\n",
        "            self.blocks.append(block)\n",
        "\n",
        "        # # decoder\n",
        "        # decoder_hidden_dim = int(self.embed_dim * mlp_ratio)\n",
        "        # self.decoder = MLP(in_features = self.embed_dim + self.big_skip*self.in_chans,\n",
        "        #                    out_features = self.out_chans,\n",
        "        #                    hidden_features = decoder_hidden_dim,\n",
        "        #                    act_layer = self.activation_function,\n",
        "        #                    drop_rate = drop_rate,\n",
        "        #                    checkpointing = False)\n",
        "\n",
        "        # construct an decoder with num_decoder_layers\n",
        "        num_decoder_layers = 1\n",
        "        decoder_hidden_dim = int(self.embed_dim * mlp_ratio)\n",
        "        current_dim = self.embed_dim + self.big_skip*self.in_chans\n",
        "        decoder_layers = []\n",
        "        for l in range(num_decoder_layers-1):\n",
        "            fc = nn.Conv2d(current_dim, decoder_hidden_dim, 1, bias=True)\n",
        "            # initialize the weights correctly\n",
        "            scale = math.sqrt(2. / current_dim)\n",
        "            nn.init.normal_(fc.weight, mean=0., std=scale)\n",
        "            if fc.bias is not None:\n",
        "                nn.init.constant_(fc.bias, 0.0)\n",
        "            decoder_layers.append(fc)\n",
        "            decoder_layers.append(self.activation_function())\n",
        "            current_dim = decoder_hidden_dim\n",
        "        fc = nn.Conv2d(current_dim, self.out_chans, 1, bias=False)\n",
        "        scale = math.sqrt(1. / current_dim)\n",
        "        nn.init.normal_(fc.weight, mean=0., std=scale)\n",
        "        if fc.bias is not None:\n",
        "            nn.init.constant_(fc.bias, 0.0)\n",
        "        decoder_layers.append(fc)\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {\"pos_embed\", \"cls_token\"}\n",
        "\n",
        "    def forward_features(self, x):\n",
        "\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if self.big_skip:\n",
        "            residual = x\n",
        "\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        if self.pos_embed is not None:\n",
        "            x = x + self.pos_embed\n",
        "\n",
        "        x = self.forward_features(x)\n",
        "\n",
        "        if self.big_skip:\n",
        "            x = torch.cat((x, residual), dim=1)\n",
        "\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "UPBfb6GTlfPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlat = 128\n",
        "nlon = 256\n",
        "model = SphericalFourierNeuralOperatorNet(spectral_transform='sht', operator_type='driscoll-healy', img_size=(nlat, nlon), in_chans=2, out_chans=1, grid=\"equiangular\",\n",
        "                                          num_layers=4, scale_factor=3, embed_dim=16, big_skip=True, pos_embed=\"lat\", use_mlp=False, normalization_layer=\"none\").to(device)"
      ],
      "metadata": {
        "id": "4Y7KocPzljgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.randn(1, 2, nlat, nlon)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8WGewCVl9Pb",
        "outputId": "a1b4b084-5f1b-4d82-924f-b9cc986c89f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 128, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9qmMOvrDpcfE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}